{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d678fe6-5977-4252-87c9-ef51d41ce67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping segment 166 due to insufficient length: 0 samples\n",
      "Skipping segment 167 due to insufficient length: 0 samples\n",
      "Skipping segment 304 due to insufficient length: 0 samples\n",
      "Skipping segment 401 due to insufficient length: 0 samples\n",
      "Skipping segment 466 due to insufficient length: 0 samples\n",
      "Skipping segment 467 due to insufficient length: 0 samples\n",
      "Skipping segment 468 due to insufficient length: 0 samples\n",
      "Skipping segment 469 due to insufficient length: 0 samples\n",
      "Skipping segment 470 due to insufficient length: 0 samples\n",
      "Skipping segment 471 due to insufficient length: 0 samples\n",
      "Skipping segment 472 due to insufficient length: 0 samples\n",
      "Skipping segment 473 due to insufficient length: 0 samples\n",
      "Skipping segment 474 due to insufficient length: 0 samples\n",
      "Skipping segment 475 due to insufficient length: 0 samples\n",
      "Skipping segment 476 due to insufficient length: 0 samples\n",
      "Skipping segment 477 due to insufficient length: 0 samples\n",
      "Skipping segment 478 due to insufficient length: 0 samples\n",
      "Skipping segment 479 due to insufficient length: 0 samples\n",
      "Skipping segment 480 due to insufficient length: 0 samples\n",
      "Skipping segment 481 due to insufficient length: 0 samples\n",
      "Skipping segment 482 due to insufficient length: 0 samples\n",
      "Skipping segment 483 due to insufficient length: 0 samples\n",
      "Skipping segment 484 due to insufficient length: 0 samples\n",
      "Skipping segment 485 due to insufficient length: 0 samples\n",
      "Skipping segment 486 due to insufficient length: 0 samples\n",
      "Skipping segment 487 due to insufficient length: 0 samples\n",
      "Skipping segment 488 due to insufficient length: 0 samples\n",
      "Skipping segment 489 due to insufficient length: 0 samples\n",
      "Skipping segment 490 due to insufficient length: 0 samples\n",
      "Skipping segment 491 due to insufficient length: 0 samples\n",
      "Skipping segment 525 due to insufficient length: 0 samples\n",
      "Skipping segment 526 due to insufficient length: 0 samples\n",
      "Skipping segment 527 due to insufficient length: 0 samples\n",
      "Skipping segment 528 due to insufficient length: 0 samples\n",
      "Skipping segment 529 due to insufficient length: 0 samples\n",
      "Skipping segment 530 due to insufficient length: 0 samples\n",
      "Skipping segment 531 due to insufficient length: 0 samples\n",
      "Skipping segment 532 due to insufficient length: 0 samples\n",
      "Skipping segment 533 due to insufficient length: 0 samples\n",
      "Skipping segment 534 due to insufficient length: 0 samples\n",
      "Skipping segment 535 due to insufficient length: 0 samples\n",
      "Skipping segment 536 due to insufficient length: 0 samples\n",
      "Skipping segment 537 due to insufficient length: 0 samples\n",
      "Skipping segment 538 due to insufficient length: 0 samples\n",
      "Skipping segment 539 due to insufficient length: 0 samples\n",
      "Skipping segment 540 due to insufficient length: 0 samples\n",
      "Skipping segment 541 due to insufficient length: 0 samples\n",
      "Skipping segment 542 due to insufficient length: 0 samples\n",
      "Skipping segment 543 due to insufficient length: 0 samples\n",
      "Skipping segment 544 due to insufficient length: 0 samples\n",
      "Skipping segment 654 due to insufficient length: 0 samples\n",
      "Skipping segment 655 due to insufficient length: 0 samples\n",
      "Skipping segment 656 due to insufficient length: 0 samples\n",
      "Skipping segment 657 due to insufficient length: 0 samples\n",
      "Skipping segment 658 due to insufficient length: 0 samples\n",
      "Skipping segment 659 due to insufficient length: 0 samples\n",
      "Skipping segment 660 due to insufficient length: 0 samples\n",
      "Skipping segment 661 due to insufficient length: 0 samples\n",
      "Skipping segment 662 due to insufficient length: 0 samples\n",
      "Skipping segment 663 due to insufficient length: 0 samples\n",
      "Skipping segment 664 due to insufficient length: 0 samples\n",
      "Skipping segment 665 due to insufficient length: 0 samples\n",
      "Skipping segment 666 due to insufficient length: 0 samples\n",
      "Skipping segment 667 due to insufficient length: 0 samples\n",
      "Skipping segment 714 due to insufficient length: 0 samples\n",
      "Skipping segment 715 due to insufficient length: 0 samples\n",
      "Skipping segment 728 due to insufficient length: 0 samples\n",
      "Skipping segment 1140 due to insufficient length: 0 samples\n",
      "Skipping segment 1141 due to insufficient length: 0 samples\n",
      "Skipping segment 1142 due to insufficient length: 0 samples\n",
      "Skipping segment 1143 due to insufficient length: 0 samples\n",
      "Skipping segment 1199 due to insufficient length: 0 samples\n",
      "Skipping segment 1200 due to insufficient length: 0 samples\n",
      "Skipping segment 1201 due to insufficient length: 0 samples\n",
      "Skipping segment 1202 due to insufficient length: 0 samples\n",
      "Skipping segment 1203 due to insufficient length: 0 samples\n",
      "Skipping segment 1213 due to insufficient length: 0 samples\n",
      "Skipping segment 1214 due to insufficient length: 0 samples\n",
      "Skipping segment 1215 due to insufficient length: 0 samples\n",
      "Skipping segment 1216 due to insufficient length: 0 samples\n",
      "Skipping segment 1258 due to insufficient length: 0 samples\n",
      "Skipping segment 1259 due to insufficient length: 0 samples\n",
      "Skipping segment 1260 due to insufficient length: 0 samples\n",
      "Skipping segment 1261 due to insufficient length: 0 samples\n",
      "Skipping segment 1262 due to insufficient length: 0 samples\n",
      "Skipping segment 1263 due to insufficient length: 0 samples\n",
      "Skipping segment 1264 due to insufficient length: 0 samples\n",
      "Skipping segment 1265 due to insufficient length: 0 samples\n",
      "Skipping segment 1266 due to insufficient length: 0 samples\n",
      "Skipping segment 1267 due to insufficient length: 0 samples\n",
      "Skipping segment 1268 due to insufficient length: 0 samples\n",
      "Skipping segment 1269 due to insufficient length: 0 samples\n",
      "Skipping segment 1317 due to insufficient length: 0 samples\n",
      "Skipping segment 1318 due to insufficient length: 0 samples\n",
      "Skipping segment 1319 due to insufficient length: 0 samples\n",
      "Skipping segment 1320 due to insufficient length: 0 samples\n",
      "Skipping segment 1321 due to insufficient length: 0 samples\n",
      "Skipping segment 1322 due to insufficient length: 0 samples\n",
      "Skipping segment 1323 due to insufficient length: 0 samples\n",
      "Skipping segment 1324 due to insufficient length: 0 samples\n",
      "Skipping segment 1325 due to insufficient length: 0 samples\n",
      "Skipping segment 1326 due to insufficient length: 0 samples\n",
      "Skipping segment 1327 due to insufficient length: 0 samples\n",
      "Skipping segment 1417 due to insufficient length: 0 samples\n",
      "Skipping segment 1418 due to insufficient length: 0 samples\n",
      "Skipping segment 1419 due to insufficient length: 0 samples\n",
      "Skipping segment 1420 due to insufficient length: 0 samples\n",
      "Skipping segment 1421 due to insufficient length: 0 samples\n",
      "Skipping segment 1422 due to insufficient length: 0 samples\n",
      "Total segments skipped: 109 out of 1423\n",
      "Class distribution:\n",
      "Class 0 (Field pause): 21 samples\n",
      "Class 1 (Filled pause): 1293 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "# Create directories for saving results\n",
    "os.makedirs('results/tables', exist_ok=True)\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "# Global storage for results and models\n",
    "all_results = []\n",
    "best_models = {}\n",
    "\n",
    "# Create directories for saving results\n",
    "os.makedirs('results/tables', exist_ok=True)\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "# Paths to JSON and audio files\n",
    "json_folder = 'JSON'\n",
    "audio_folder = './output/cleaned_wav_files'\n",
    "\n",
    "# Match audio and JSON files\n",
    "json_files = {os.path.splitext(f)[0]: os.path.join(json_folder, f) for f in os.listdir(json_folder) if f.endswith('.json')}\n",
    "audio_files = {os.path.splitext(f)[0]: os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(('.wav', '.m4a', '.mp3'))}\n",
    "matched_files = {name: (json_files[name], audio_files[name]) for name in json_files if name in audio_files}\n",
    "\n",
    "# Function to load annotations from a JSON file\n",
    "def load_annotations(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    annotations = []\n",
    "    for item in data[0]['annotations'][0]['result']:\n",
    "        if item['type'] == 'labels':\n",
    "            start = item['value']['start']\n",
    "            end = item['value']['end']\n",
    "            label = item['value']['labels'][0]\n",
    "            annotations.append((start, end, label))\n",
    "    return annotations\n",
    "\n",
    "# Function to extract audio segments based on annotations\n",
    "def extract_audio_segments(audio_file, annotations, sr=16000):\n",
    "    y, _ = librosa.load(audio_file, sr=sr)\n",
    "    segments = []\n",
    "    for start, end, label in annotations:\n",
    "        segment = y[int(start * sr):int(end * sr)]\n",
    "        segments.append((segment, label))\n",
    "    return segments\n",
    "\n",
    "# Function to extract features (MFCC) from audio segments with improved handling of short segments\n",
    "def extract_features(segments, n_mfcc=40, max_length=300, min_segment_length=512):\n",
    "    features, labels = [], []\n",
    "    skipped_segments = 0\n",
    "    \n",
    "    for i, (segment, label) in enumerate(segments):\n",
    "        try:\n",
    "            # Check if the segment is too short for any FFT processing\n",
    "            if len(segment) < min_segment_length:\n",
    "                print(f\"Skipping segment {i} due to insufficient length: {len(segment)} samples\")\n",
    "                skipped_segments += 1\n",
    "                continue\n",
    "                \n",
    "            # For very short segments, use a smaller n_fft and hop_length\n",
    "            if len(segment) < 2048:\n",
    "                n_fft = 512\n",
    "                hop_length = 128\n",
    "            else:\n",
    "                n_fft = 2048\n",
    "                hop_length = 512\n",
    "                \n",
    "            # Extract MFCC features with adjusted parameters\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=segment, \n",
    "                sr=16000, \n",
    "                n_mfcc=n_mfcc, \n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length\n",
    "            )\n",
    "            \n",
    "            # Handle variable lengths (pad if short, truncate if long)\n",
    "            if mfcc.shape[1] < max_length:\n",
    "                padded_mfcc = np.pad(mfcc, ((0, 0), (0, max_length - mfcc.shape[1])), mode='constant')\n",
    "            else:\n",
    "                padded_mfcc = mfcc[:, :max_length]\n",
    "                \n",
    "            # Append features and labels\n",
    "            features.append(padded_mfcc.T)\n",
    "            labels.append(0 if label == 'Field pause' else 1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {i}: {e}\")\n",
    "            skipped_segments += 1\n",
    "    \n",
    "    print(f\"Total segments skipped: {skipped_segments} out of {len(segments)}\")\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = []\n",
    "files_loaded = []\n",
    "\n",
    "for name, (json_path, audio_path) in matched_files.items():\n",
    "    annotations = load_annotations(json_path)\n",
    "    if len(annotations) != 0:\n",
    "        files_loaded.append(json_path)\n",
    "        audio_segments = extract_audio_segments(audio_path, annotations)\n",
    "        dataset.extend(audio_segments)\n",
    "\n",
    "# Extract features with improved handling of short segments\n",
    "X, y = extract_features(dataset, min_segment_length=256)  # Lowered minimum segment length\n",
    "\n",
    "# Display class distribution\n",
    "print(\"Class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label} ({'Field pause' if label == 0 else 'Filled pause'}): {count} samples\")\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Class Distribution in Dataset')\n",
    "plt.xlabel('Class (0: Field pause, 1: Filled pause)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('results/plots/class_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Make a copy of the original train/test data\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "y_test_original = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea47b5f-cb3b-4f9d-914c-e0cee788c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting filled pause detection model evaluation with imbalanced data handling...\n",
      "\n",
      "Data distribution analysis:\n",
      "Class 0 (Field pause): 21 samples (1.60%)\n",
      "Class 1 (Filled pause): 1293 samples (98.40%)\n",
      "\n",
      "Creating balanced datasets from 1051 samples\n",
      "Original class distribution: {1: 1034, 0: 17}\n",
      "Undersampled dataset: 51 samples, distribution: {0: 17, 1: 34}\n",
      "1-shot dataset: 2 samples, distribution: {0: 1, 1: 1}\n",
      "3-shot dataset: 6 samples, distribution: {0: 3, 1: 3}\n",
      "5-shot dataset: 10 samples, distribution: {0: 5, 1: 5}\n",
      "10-shot dataset: 20 samples, distribution: {0: 10, 1: 10}\n",
      "17-shot dataset: 34 samples, distribution: {0: 17, 1: 17}\n",
      "\n",
      "\n",
      "==== Phase 1: Training Specialized Models for Imbalanced Data ====\n",
      "\n",
      "Training Balanced Random Forest (full)...\n",
      "\n",
      "BalancedRF (full) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9979\n",
      "Best threshold: 0.2700\n",
      "Execution Time: 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved to results/model_architecture/BalancedRF_full.txt\n",
      "\n",
      "Training RUSBoost (full)...\n",
      "\n",
      "RUSBoost (full) Results:\n",
      "Accuracy: 0.9924\n",
      "Precision: 0.9923\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9962\n",
      "Average Precision (AP): 0.9932\n",
      "Best threshold: 0.3980\n",
      "Execution Time: 0.17 seconds\n",
      "Model architecture saved to results/model_architecture/RUSBoost_full.txt\n",
      "\n",
      "Training Cost-sensitive SVM (full)...\n",
      "\n",
      "WeightedSVM (full) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9977\n",
      "Best threshold: 0.9815\n",
      "Execution Time: 1.79 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedSVM_full.txt\n",
      "\n",
      "Training Weighted XGBoost (full)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:42:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WeightedXGB (full) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9952\n",
      "Best threshold: 0.4423\n",
      "Execution Time: 0.13 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedXGB_full.txt\n",
      "\n",
      "\n",
      "==== Phase 2: Training with Undersampled Balanced Dataset ====\n",
      "\n",
      "Training k-NN (balanced)...\n",
      "Best k for balanced: 11\n",
      "\n",
      "kNN (balanced) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9925\n",
      "Best threshold: 0.3636\n",
      "Execution Time: 0.08 seconds\n",
      "Model architecture saved to results/model_architecture/kNN_balanced.txt\n",
      "\n",
      "Training Random Forest with RFE (balanced)...\n",
      "\n",
      "RF_RFE (balanced) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9924\n",
      "Best threshold: 0.3100\n",
      "Execution Time: 0.05 seconds\n",
      "Model architecture saved to results/model_architecture/RF_RFE_balanced.txt\n",
      "\n",
      "Training SVM (balanced)...\n",
      "\n",
      "SVM (balanced) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9525\n",
      "Best threshold: 0.6448\n",
      "Execution Time: 0.30 seconds\n",
      "Model architecture saved to results/model_architecture/SVM_balanced.txt\n",
      "\n",
      "Training Deep Learning Models one by one to manage memory...\n",
      "\n",
      "Training MLP (balanced)...\n",
      "Model architecture saved to results/model_architecture/MLP_balanced.txt\n",
      "9/9 [==============================] - 0s 978us/step\n",
      "\n",
      "MLP (balanced) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Execution Time: 0.07 seconds\n",
      "\n",
      "Training CNN (balanced)...\n",
      "Model architecture saved to results/model_architecture/CNN_balanced.txt\n",
      "\n",
      "CNN (balanced) Results:\n",
      "Accuracy: 0.7643\n",
      "Precision: 1.0000\n",
      "Recall: 0.7606\n",
      "F1 Score: 0.8640\n",
      "Execution Time: 0.08 seconds\n",
      "\n",
      "Training CNN+XGBoost (balanced)...\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x34e1434c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Model architecture saved to results/model_architecture/CNN_XGBoost_balanced.txt\n",
      "WARNING:tensorflow:6 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x34e1400e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:44:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN_XGBoost (balanced) Results:\n",
      "Accuracy: 0.6400\n",
      "Precision: 1.0000\n",
      "Recall: 0.6327\n",
      "F1 Score: 0.7750\n",
      "Execution Time: 0.21 seconds\n",
      "\n",
      "Training LSTM (balanced)...\n",
      "Model architecture saved to results/model_architecture/LSTM_balanced.txt\n",
      "\n",
      "LSTM (balanced) Results:\n",
      "Accuracy: 0.9800\n",
      "Precision: 0.9800\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9899\n",
      "Execution Time: 0.50 seconds\n",
      "\n",
      "\n",
      "==== Phase 3: Training with Few-Shot Learning ====\n",
      "\n",
      "--- Training with 3-shot Learning ---\n",
      "\n",
      "Training k-NN (3-shot)...\n",
      "\n",
      "kNN (3-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9833\n",
      "Best threshold: 0.4000\n",
      "Execution Time: 0.06 seconds\n",
      "Model architecture saved to results/model_architecture/kNN_3-shot.txt\n",
      "\n",
      "Training Random Forest with RFE (3-shot)...\n",
      "Error in RF+RFE: Not enough samples for RFE\n",
      "Falling back to standard Random Forest\n",
      "\n",
      "RF (3-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9975\n",
      "Best threshold: 0.3100\n",
      "Execution Time: 0.03 seconds\n",
      "Model architecture saved to results/model_architecture/RF_3-shot.txt\n",
      "\n",
      "Training SVM (3-shot)...\n",
      "\n",
      "SVM (3-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9893\n",
      "Best threshold: 0.4859\n",
      "Execution Time: 0.10 seconds\n",
      "Model architecture saved to results/model_architecture/SVM_3-shot.txt\n",
      "\n",
      "--- Training with 5-shot Learning ---\n",
      "\n",
      "Training k-NN (5-shot)...\n",
      "\n",
      "kNN (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9839\n",
      "Best threshold: 0.2000\n",
      "Execution Time: 0.04 seconds\n",
      "Model architecture saved to results/model_architecture/kNN_5-shot.txt\n",
      "\n",
      "Training Random Forest with RFE (5-shot)...\n",
      "\n",
      "RF_RFE (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9775\n",
      "Best threshold: 0.1200\n",
      "Execution Time: 0.05 seconds\n",
      "Model architecture saved to results/model_architecture/RF_RFE_5-shot.txt\n",
      "\n",
      "Training SVM (5-shot)...\n",
      "\n",
      "SVM (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9795\n",
      "Best threshold: 0.4483\n",
      "Execution Time: 0.12 seconds\n",
      "Model architecture saved to results/model_architecture/SVM_5-shot.txt\n",
      "\n",
      "Training Balanced Random Forest (5-shot)...\n",
      "\n",
      "BalancedRF (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9903\n",
      "Best threshold: 0.3000\n",
      "Execution Time: 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved to results/model_architecture/BalancedRF_5-shot.txt\n",
      "\n",
      "Training RUSBoost (5-shot)...\n",
      "\n",
      "RUSBoost (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9833\n",
      "Best threshold: 0.0000\n",
      "Execution Time: 0.01 seconds\n",
      "Model architecture saved to results/model_architecture/RUSBoost_5-shot.txt\n",
      "\n",
      "Training Cost-sensitive SVM (5-shot)...\n",
      "\n",
      "WeightedSVM (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9795\n",
      "Best threshold: 0.4483\n",
      "Execution Time: 0.12 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedSVM_5-shot.txt\n",
      "\n",
      "Training Weighted XGBoost (5-shot)...\n",
      "\n",
      "WeightedXGB (5-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9888\n",
      "Best threshold: 0.2743\n",
      "Execution Time: 0.01 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedXGB_5-shot.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:45:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with 10-shot Learning ---\n",
      "\n",
      "Training k-NN (10-shot)...\n",
      "Best k for 10-shot: 3\n",
      "\n",
      "kNN (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9887\n",
      "Execution Time: 0.07 seconds\n",
      "Model architecture saved to results/model_architecture/kNN_10-shot.txt\n",
      "\n",
      "Training Random Forest with RFE (10-shot)...\n",
      "\n",
      "RF_RFE (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9908\n",
      "Best threshold: 0.1700\n",
      "Execution Time: 0.05 seconds\n",
      "Model architecture saved to results/model_architecture/RF_RFE_10-shot.txt\n",
      "\n",
      "Training SVM (10-shot)...\n",
      "\n",
      "SVM (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9777\n",
      "Best threshold: 0.3970\n",
      "Execution Time: 0.15 seconds\n",
      "Model architecture saved to results/model_architecture/SVM_10-shot.txt\n",
      "\n",
      "Training Balanced Random Forest (10-shot)...\n",
      "\n",
      "BalancedRF (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9954\n",
      "Best threshold: 0.1800\n",
      "Execution Time: 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved to results/model_architecture/BalancedRF_10-shot.txt\n",
      "\n",
      "Training RUSBoost (10-shot)...\n",
      "\n",
      "RUSBoost (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9822\n",
      "Best threshold: 0.0000\n",
      "Execution Time: 0.11 seconds\n",
      "Model architecture saved to results/model_architecture/RUSBoost_10-shot.txt\n",
      "\n",
      "Training Cost-sensitive SVM (10-shot)...\n",
      "\n",
      "WeightedSVM (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9777\n",
      "Best threshold: 0.3970\n",
      "Execution Time: 0.16 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedSVM_10-shot.txt\n",
      "\n",
      "Training Weighted XGBoost (10-shot)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:47:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WeightedXGB (10-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9864\n",
      "Best threshold: 0.0975\n",
      "Execution Time: 0.11 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedXGB_10-shot.txt\n",
      "\n",
      "Training MLP (10-shot)...\n",
      "Model architecture saved to results/model_architecture/MLP_10-shot.txt\n",
      "9/9 [==============================] - 0s 990us/step\n",
      "\n",
      "MLP (10-shot) Results:\n",
      "Accuracy: 0.8821\n",
      "Precision: 0.9872\n",
      "Recall: 0.8919\n",
      "F1 Score: 0.9371\n",
      "Execution Time: 0.06 seconds\n",
      "\n",
      "Training CNN (10-shot)...\n",
      "Model architecture saved to results/model_architecture/CNN_10-shot.txt\n",
      "\n",
      "CNN (10-shot) Results:\n",
      "Accuracy: 0.7034\n",
      "Precision: 0.9840\n",
      "Recall: 0.7104\n",
      "F1 Score: 0.8251\n",
      "Execution Time: 0.08 seconds\n",
      "\n",
      "Training LSTM (10-shot)...\n",
      "Model architecture saved to results/model_architecture/LSTM_10-shot.txt\n",
      "\n",
      "LSTM (10-shot) Results:\n",
      "Accuracy: 0.2850\n",
      "Precision: 1.0000\n",
      "Recall: 0.2704\n",
      "F1 Score: 0.4257\n",
      "Execution Time: 0.50 seconds\n",
      "\n",
      "--- Training with 17-shot Learning ---\n",
      "\n",
      "Training k-NN (17-shot)...\n",
      "Best k for 17-shot: 5\n",
      "\n",
      "kNN (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9885\n",
      "Execution Time: 0.05 seconds\n",
      "Model architecture saved to results/model_architecture/kNN_17-shot.txt\n",
      "\n",
      "Training Random Forest with RFE (17-shot)...\n",
      "\n",
      "RF_RFE (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9893\n",
      "Best threshold: 0.0500\n",
      "Execution Time: 0.06 seconds\n",
      "Model architecture saved to results/model_architecture/RF_RFE_17-shot.txt\n",
      "\n",
      "Training SVM (17-shot)...\n",
      "\n",
      "SVM (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9957\n",
      "Best threshold: 0.4282\n",
      "Execution Time: 0.21 seconds\n",
      "Model architecture saved to results/model_architecture/SVM_17-shot.txt\n",
      "\n",
      "Training Balanced Random Forest (17-shot)...\n",
      "\n",
      "BalancedRF (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9954\n",
      "Best threshold: 0.1500\n",
      "Execution Time: 0.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved to results/model_architecture/BalancedRF_17-shot.txt\n",
      "\n",
      "Training RUSBoost (17-shot)...\n",
      "\n",
      "RUSBoost (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9941\n",
      "Best threshold: 0.0001\n",
      "Execution Time: 0.17 seconds\n",
      "Model architecture saved to results/model_architecture/RUSBoost_17-shot.txt\n",
      "\n",
      "Training Cost-sensitive SVM (17-shot)...\n",
      "\n",
      "WeightedSVM (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9957\n",
      "Best threshold: 0.4282\n",
      "Execution Time: 0.21 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedSVM_17-shot.txt\n",
      "\n",
      "Training Weighted XGBoost (17-shot)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:49:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WeightedXGB (17-shot) Results:\n",
      "Accuracy: 0.9848\n",
      "Precision: 0.9848\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9923\n",
      "Average Precision (AP): 0.9923\n",
      "Best threshold: 0.0504\n",
      "Execution Time: 0.13 seconds\n",
      "Model architecture saved to results/model_architecture/WeightedXGB_17-shot.txt\n",
      "\n",
      "Training MLP (17-shot)...\n",
      "Model architecture saved to results/model_architecture/MLP_17-shot.txt\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "\n",
      "MLP (17-shot) Results:\n",
      "Accuracy: 0.9582\n",
      "Precision: 0.9844\n",
      "Recall: 0.9730\n",
      "F1 Score: 0.9786\n",
      "Execution Time: 0.06 seconds\n",
      "\n",
      "Training CNN (17-shot)...\n",
      "Model architecture saved to results/model_architecture/CNN_17-shot.txt\n",
      "\n",
      "CNN (17-shot) Results:\n",
      "Accuracy: 0.1445\n",
      "Precision: 1.0000\n",
      "Recall: 0.1313\n",
      "F1 Score: 0.2321\n",
      "Execution Time: 0.08 seconds\n",
      "\n",
      "Training LSTM (17-shot)...\n",
      "Model architecture saved to results/model_architecture/LSTM_17-shot.txt\n",
      "\n",
      "LSTM (17-shot) Results:\n",
      "Accuracy: 0.2600\n",
      "Precision: 1.0000\n",
      "Recall: 0.2449\n",
      "F1 Score: 0.3934\n",
      "Execution Time: 0.50 seconds\n",
      "\n",
      "\n",
      "==== Phase 4: Analyzing Results ====\n",
      "\n",
      "Best model: RUSBoost_full with F1 score: 0.9962\n",
      "\n",
      "\n",
      "==== Phase 5: Applying XAI to Top Models Only ====\n",
      "\n",
      "Creating visualizations for RUSBoost (full)...\n",
      "\n",
      "Applying XAI to RUSBoost (full)...\n",
      "\n",
      "Performance metrics for RUSBoost (full):\n",
      "Accuracy: 0.9500\n",
      "Precision: 0.9792\n",
      "Recall: 0.9691\n",
      "F1 Score: 0.9741\n",
      "LIME explanations created for RUSBoost\n",
      "\n",
      "Creating visualizations for BalancedRF (full)...\n",
      "\n",
      "Applying XAI to BalancedRF (full)...\n",
      "\n",
      "Performance metrics for BalancedRF (full):\n",
      "Accuracy: 0.5700\n",
      "Precision: 1.0000\n",
      "Recall: 0.5567\n",
      "F1 Score: 0.7152\n",
      "LIME explanations created for BalancedRF\n",
      "\n",
      "Creating visualizations for RUSBoost (5-shot)...\n",
      "\n",
      "Applying XAI to RUSBoost (5-shot)...\n",
      "\n",
      "Performance metrics for RUSBoost (5-shot):\n",
      "Accuracy: 0.4200\n",
      "Precision: 0.9756\n",
      "Recall: 0.4124\n",
      "F1 Score: 0.5797\n",
      "LIME explanations created for RUSBoost\n",
      "\n",
      "XAI analysis complete. Results saved to results/tables/xai_performance.csv\n",
      "XAI visualizations saved to results/plots/ directory\n",
      "\n",
      "All processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate models for imbalanced datasets\n",
    "def evaluate_model_imbalanced(model, X_test, y_test, model_name, shot_config=\"full\"):\n",
    "    \"\"\"\n",
    "    Evaluate a model with metrics suitable for imbalanced datasets\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reshape data if needed\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0], -1) if len(X_test.shape) > 2 else X_test\n",
    "    \n",
    "    # For models that return probabilities\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            y_prob = model.predict_proba(X_test_reshaped)[:, 1]\n",
    "            \n",
    "            # Calculate precision-recall curve\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "            \n",
    "            # Find threshold that maximizes F1 score - limit to max 100 thresholds for efficiency\n",
    "            if len(thresholds) > 100:\n",
    "                # Sample evenly spaced thresholds\n",
    "                indices = np.linspace(0, len(thresholds)-1, 100, dtype=int)\n",
    "                thresholds_sample = thresholds[indices]\n",
    "            else:\n",
    "                thresholds_sample = thresholds\n",
    "                \n",
    "            f1_scores = []\n",
    "            for t in thresholds_sample:\n",
    "                y_pred_t = (y_prob >= t).astype(int)\n",
    "                f1_scores.append(f1_score(y_test, y_pred_t))\n",
    "                \n",
    "            best_threshold_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds_sample[best_threshold_idx]\n",
    "            \n",
    "            # Calculate predictions with optimal threshold\n",
    "            y_pred = (y_prob >= best_threshold).astype(int)\n",
    "            \n",
    "            # Calculate AUC-PR (Area Under Precision-Recall Curve)\n",
    "            average_precision = average_precision_score(y_test, y_prob)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating probabilities: {e}\")\n",
    "            y_pred = model.predict(X_test_reshaped)\n",
    "            y_pred = np.round(y_pred).astype(int)  # Convert continuous to binary\n",
    "            average_precision = None\n",
    "            best_threshold = None\n",
    "    else:\n",
    "        # If no probability predictions available\n",
    "        y_pred = model.predict(X_test_reshaped)\n",
    "        # Handle Keras models that output continuous values\n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim > 1 and y_pred.shape[1] == 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        y_pred = np.round(y_pred).astype(int)  # Convert to binary\n",
    "        average_precision = None\n",
    "        best_threshold = None\n",
    "    \n",
    "    # Calculate standard metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} ({shot_config}) Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    if average_precision:\n",
    "        print(f\"Average Precision (AP): {average_precision:.4f}\")\n",
    "    if best_threshold:\n",
    "        print(f\"Best threshold: {best_threshold:.4f}\")\n",
    "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'model_name': f\"{model_name}_{shot_config}\",\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'average_precision': average_precision,\n",
    "        'best_threshold': best_threshold,\n",
    "        'confusion_matrix': cm,\n",
    "        'execution_time': execution_time,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to plot and save confusion matrix - FUNCTION KEPT BUT NO LONGER USED\n",
    "def plot_confusion_matrix(cm, model_name, shot_config):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix\n",
    "    \"\"\"\n",
    "    # REMOVED: Plotting and saving confusion matrix\n",
    "    pass\n",
    "\n",
    "# Function to save model architecture\n",
    "def save_model_architecture(model, model_name, shot_config):\n",
    "    \"\"\"\n",
    "    Save the architecture of a model to a text file\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    architecture_dir = 'results/model_architecture'\n",
    "    os.makedirs(architecture_dir, exist_ok=True)\n",
    "    \n",
    "    file_path = f'{architecture_dir}/{model_name}_{shot_config}.txt'\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            # Handle different model types\n",
    "            if 'keras' in str(type(model)).lower() or 'tensorflow' in str(type(model)).lower():\n",
    "                # For Keras/TensorFlow models\n",
    "                model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "            elif hasattr(model, 'get_params'):\n",
    "                # For scikit-learn models\n",
    "                f.write(f\"Model Type: {type(model).__name__}\\n\")\n",
    "                f.write(\"Parameters:\\n\")\n",
    "                for param, value in model.get_params().items():\n",
    "                    f.write(f\"  {param}: {value}\\n\")\n",
    "            elif isinstance(model, dict) and 'model' in model:\n",
    "                # For wrapped models\n",
    "                if 'keras' in str(type(model['model'])).lower() or 'tensorflow' in str(type(model['model'])).lower():\n",
    "                    model['model'].summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "                elif hasattr(model['model'], 'get_params'):\n",
    "                    f.write(f\"Model Type: {type(model['model']).__name__}\\n\")\n",
    "                    f.write(\"Parameters:\\n\")\n",
    "                    for param, value in model['model'].get_params().items():\n",
    "                        f.write(f\"  {param}: {value}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"Model Type: {type(model['model']).__name__}\\n\")\n",
    "            else:\n",
    "                # For other model types\n",
    "                f.write(f\"Model Type: {type(model).__name__}\\n\")\n",
    "        \n",
    "        print(f\"Model architecture saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model architecture: {e}\")\n",
    "\n",
    "# Function to create balanced datasets\n",
    "def create_balanced_datasets(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Create balanced datasets for training using different techniques\"\"\"\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate class distribution\n",
    "    class_counts = Counter(y_train)\n",
    "    minority_class = min(class_counts, key=class_counts.get)\n",
    "    minority_count = class_counts[minority_class]\n",
    "    \n",
    "    print(f\"\\nCreating balanced datasets from {len(y_train)} samples\")\n",
    "    print(f\"Original class distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    balanced_datasets = {}\n",
    "    \n",
    "    # 1. Random Undersampling - use all minority samples, randomly select majority samples\n",
    "    majority_indices = np.where(y_train != minority_class)[0]\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    \n",
    "    # Randomly select majority samples equal to minority count * factor\n",
    "    undersampling_factor = 2  # Use 2x minority samples from majority class\n",
    "    selected_majority = np.random.choice(\n",
    "        majority_indices, \n",
    "        size=min(minority_count * undersampling_factor, len(majority_indices)),\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # Combine indices and create balanced dataset\n",
    "    balanced_indices = np.concatenate([minority_indices, selected_majority])\n",
    "    X_balanced = X_train[balanced_indices]\n",
    "    y_balanced = y_train[balanced_indices]\n",
    "    \n",
    "    balanced_datasets['undersampled'] = (X_balanced, y_balanced)\n",
    "    \n",
    "    # Print the new distribution\n",
    "    print(f\"Undersampled dataset: {len(X_balanced)} samples, distribution: {dict(Counter(y_balanced))}\")\n",
    "    \n",
    "    # 2. Create few-shot learning datasets\n",
    "    minority_shots = min(minority_count, 20)  # Cap at 20 samples\n",
    "    shot_configs = [1, 3, 5, 10, minority_shots]\n",
    "    \n",
    "    for n_shots in shot_configs:\n",
    "        if n_shots <= minority_count:\n",
    "            X_few, y_few = create_few_shot_dataset(X_train, y_train, n_shots)\n",
    "            balanced_datasets[f'{n_shots}-shot'] = (X_few, y_few)\n",
    "            print(f\"{n_shots}-shot dataset: {len(X_few)} samples, distribution: {dict(Counter(y_few))}\")\n",
    "    \n",
    "    return balanced_datasets\n",
    "\n",
    "# Function to create few-shot datasets\n",
    "def create_few_shot_dataset(X, y, n_shots=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a few-shot learning dataset with n examples per class\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Find indices for each class\n",
    "    unique_labels = np.unique(y)\n",
    "    few_shot_indices = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(y == label)[0]\n",
    "        n_available = len(indices)\n",
    "        selected_n = min(n_shots, n_available)\n",
    "        \n",
    "        if selected_n < n_shots:\n",
    "            print(f\"Warning: Only {selected_n} samples available for class {label}\")\n",
    "        \n",
    "        selected = np.random.choice(indices, selected_n, replace=False)\n",
    "        few_shot_indices.extend(selected)\n",
    "    \n",
    "    return X[few_shot_indices], y[few_shot_indices]\n",
    "\n",
    "# Function to train specialized models for imbalanced datasets\n",
    "def train_balanced_models(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train models specifically designed for imbalanced datasets\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    import xgboost as xgb\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "    \n",
    "    # Reshape data\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    results_list = []  # Store results locally\n",
    "    models_dict = {}   # Store models locally\n",
    "    \n",
    "    # 1. Balanced Random Forest\n",
    "    try:\n",
    "        print(f\"\\nTraining Balanced Random Forest ({shot_config})...\")\n",
    "        brf = BalancedRandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            replacement=True,\n",
    "            sampling_strategy='auto',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        brf.fit(X_train_2d, y_train)\n",
    "        results = evaluate_model_imbalanced(brf, X_test_2d, y_test, 'BalancedRF', shot_config)\n",
    "        results_list.append(results)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(brf, 'BalancedRF', shot_config)\n",
    "        \n",
    "        # Store the model\n",
    "        models_dict[\"BalancedRF\"] = brf\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error training Balanced Random Forest: {e}\")\n",
    "    \n",
    "    # 2. RUSBoost\n",
    "    try:\n",
    "        print(f\"\\nTraining RUSBoost ({shot_config})...\")\n",
    "        rusboost = RUSBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            sampling_strategy='auto',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        rusboost.fit(X_train_2d, y_train)\n",
    "        results = evaluate_model_imbalanced(rusboost, X_test_2d, y_test, 'RUSBoost', shot_config)\n",
    "        results_list.append(results)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(rusboost, 'RUSBoost', shot_config)\n",
    "        \n",
    "        # Store the model\n",
    "        models_dict[\"RUSBoost\"] = rusboost\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error training RUSBoost: {e}\")\n",
    "    \n",
    "    # 3. Cost-sensitive SVM\n",
    "    try:\n",
    "        print(f\"\\nTraining Cost-sensitive SVM ({shot_config})...\")\n",
    "        \n",
    "        # Calculate class weights inversely proportional to class frequencies\n",
    "        class_counts = Counter(y_train)\n",
    "        n_samples = len(y_train)\n",
    "        class_weights = {\n",
    "            c: n_samples / (len(class_counts) * count)\n",
    "            for c, count in class_counts.items()\n",
    "        }\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_2d)\n",
    "        X_test_scaled = scaler.transform(X_test_2d)\n",
    "        \n",
    "        # Train weighted SVM\n",
    "        svm_weighted = SVC(\n",
    "            kernel='rbf',\n",
    "            class_weight=class_weights,\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        svm_weighted.fit(X_train_scaled, y_train)\n",
    "        results = evaluate_model_imbalanced(svm_weighted, X_test_scaled, y_test, 'WeightedSVM', shot_config)\n",
    "        results_list.append(results)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(svm_weighted, 'WeightedSVM', shot_config)\n",
    "        \n",
    "        # Store the model with its scaler\n",
    "        models_dict[\"WeightedSVM\"] = {'model': svm_weighted, 'scaler': scaler}\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error training Weighted SVM: {e}\")\n",
    "    \n",
    "    # 4. XGBoost with scale_pos_weight\n",
    "    try:\n",
    "        print(f\"\\nTraining Weighted XGBoost ({shot_config})...\")\n",
    "        \n",
    "        # Calculate positive class weight\n",
    "        scale_pos_weight = class_counts[0] / class_counts[1] if 1 in class_counts else 1.0\n",
    "        \n",
    "        xgb_weighted = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        xgb_weighted.fit(X_train_2d, y_train)\n",
    "        results = evaluate_model_imbalanced(xgb_weighted, X_test_2d, y_test, 'WeightedXGB', shot_config)\n",
    "        results_list.append(results)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(xgb_weighted, 'WeightedXGB', shot_config)\n",
    "        \n",
    "        # Store the model\n",
    "        models_dict[\"WeightedXGB\"] = xgb_weighted\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error training Weighted XGBoost: {e}\")\n",
    "    \n",
    "    # Add results to global list\n",
    "    all_results.extend(results_list)\n",
    "    \n",
    "    # Add models to global dictionary\n",
    "    for model_name, model in models_dict.items():\n",
    "        best_models[f\"{model_name}_{shot_config}\"] = model\n",
    "    \n",
    "    return models_dict\n",
    "\n",
    "# Function to train k-NN model\n",
    "def train_knn(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate k-NN model\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(f\"\\nTraining k-NN ({shot_config})...\")\n",
    "    \n",
    "    # Reshape data for k-NN\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_2d)\n",
    "    X_test_scaled = scaler.transform(X_test_2d)\n",
    "    \n",
    "    try:\n",
    "        # Find best k using cross-validation\n",
    "        best_k = 5  # Default\n",
    "        if len(X_train) > 10:\n",
    "            from sklearn.model_selection import GridSearchCV\n",
    "            param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}\n",
    "            grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=min(5, len(X_train)), scoring='f1')\n",
    "            try:\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "                best_k = grid_search.best_params_['n_neighbors']\n",
    "                print(f\"Best k for {shot_config}: {best_k}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error finding best k: {e}\")\n",
    "        \n",
    "        # Train k-NN with best k\n",
    "        knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_model_imbalanced(knn, X_test_scaled, y_test, 'kNN', shot_config)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(knn, 'kNN', shot_config)\n",
    "        \n",
    "        # Save the model with its scaler\n",
    "        best_models[f\"kNN_{shot_config}\"] = {'model': knn, 'scaler': scaler}\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        return knn, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in kNN training: {e}\")\n",
    "        gc.collect()\n",
    "        return None, None\n",
    "\n",
    "# Function to train Random Forest with RFE\n",
    "def train_rf_rfe(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate Random Forest with RFE\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_selection import RFE\n",
    "    \n",
    "    print(f\"\\nTraining Random Forest with RFE ({shot_config})...\")\n",
    "    \n",
    "    # Reshape data\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    try:\n",
    "        # First train a simple random forest to use with RFE\n",
    "        base_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Apply RFE to select top features\n",
    "        # Number of features to select should be adaptive to dataset size\n",
    "        n_features = min(50, X_train_2d.shape[1])\n",
    "        \n",
    "        # Use RFE if we have enough samples\n",
    "        if len(X_train) >= 10:\n",
    "            rfe = RFE(estimator=base_rf, n_features_to_select=n_features, step=10)\n",
    "            X_train_rfe = rfe.fit_transform(X_train_2d, y_train)\n",
    "            X_test_rfe = rfe.transform(X_test_2d)\n",
    "            \n",
    "            # Train RF on selected features\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train_rfe, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            results = evaluate_model_imbalanced(rf, X_test_rfe, y_test, 'RF_RFE', shot_config)\n",
    "            all_results.append(results)\n",
    "            \n",
    "            # Save the model architecture\n",
    "            save_model_architecture(rf, 'RF_RFE', shot_config)\n",
    "            \n",
    "            # Save the model with RFE\n",
    "            best_models[f\"RF_RFE_{shot_config}\"] = {'model': rf, 'rfe': rfe}\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            return rf, results\n",
    "        else:\n",
    "            # Fall back to regular RF for very small datasets\n",
    "            raise ValueError(\"Not enough samples for RFE\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RF+RFE: {e}\")\n",
    "        print(\"Falling back to standard Random Forest\")\n",
    "        \n",
    "        try:\n",
    "            # Train standard RF\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train_2d, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            results = evaluate_model_imbalanced(rf, X_test_2d, y_test, 'RF', shot_config)\n",
    "            all_results.append(results)\n",
    "            \n",
    "            # Save the model architecture\n",
    "            save_model_architecture(rf, 'RF', shot_config)\n",
    "            \n",
    "            # Save the model\n",
    "            best_models[f\"RF_{shot_config}\"] = rf\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            return rf, results\n",
    "        except Exception as e2:\n",
    "            print(f\"Error in standard RF: {e2}\")\n",
    "            gc.collect()\n",
    "            return None, None\n",
    "\n",
    "# Function to train SVM model\n",
    "def train_svm(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate SVM model\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(f\"\\nTraining SVM ({shot_config})...\")\n",
    "    \n",
    "    try:\n",
    "        # Reshape data\n",
    "        X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # Scale features for SVM\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_2d)\n",
    "        X_test_scaled = scaler.transform(X_test_2d)\n",
    "        \n",
    "        # Train SVM with RBF kernel\n",
    "        svm = SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')\n",
    "        svm.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_model_imbalanced(svm, X_test_scaled, y_test, 'SVM', shot_config)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(svm, 'SVM', shot_config)\n",
    "        \n",
    "        # Save the model with its scaler\n",
    "        best_models[f\"SVM_{shot_config}\"] = {'model': svm, 'scaler': scaler}\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        return svm, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SVM training: {e}\")\n",
    "        gc.collect()\n",
    "        return None, None\n",
    "\n",
    "# Function to train MLP model\n",
    "def train_mlp(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate MLP model\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from collections import Counter\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    print(f\"\\nTraining MLP ({shot_config})...\")\n",
    "    \n",
    "    try:\n",
    "        # Reshape data\n",
    "        X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_2d)\n",
    "        X_test_scaled = scaler.transform(X_test_2d)\n",
    "        \n",
    "        # Build model\n",
    "        mlp = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Reduced complexity\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Set epochs based on shot configuration\n",
    "        if shot_config == \"10-shot\":\n",
    "            max_epochs = 20\n",
    "        elif shot_config == \"17-shot\":\n",
    "            max_epochs = 12\n",
    "        elif shot_config == \"5-shot\":\n",
    "            max_epochs = 20\n",
    "        elif shot_config == \"balanced\":\n",
    "            max_epochs = 16\n",
    "        else:\n",
    "            max_epochs = 30  # Reduced from 50 to prevent overfitting\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=5,  # Reduced patience \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Class weights for imbalanced data\n",
    "        class_weights = None\n",
    "        if shot_config == \"full\":\n",
    "            class_counts = Counter(y_train)\n",
    "            n_samples = len(y_train)\n",
    "            class_weights = {\n",
    "                label: n_samples / (len(class_counts) * count)\n",
    "                for label, count in class_counts.items()\n",
    "            }\n",
    "        \n",
    "        # Train model\n",
    "        batch_size = min(32, len(X_train) // 2) if len(X_train) > 1 else 1\n",
    "        history = mlp.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1 if len(X_train) > 10 else 0.0,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weights,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Plot training history with low DPI to save memory\n",
    "        dpi = 72  # Lower DPI for memory efficiency\n",
    "        plt.figure(figsize=(12, 4), dpi=dpi)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'MLP Loss ({shot_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'MLP Accuracy ({shot_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/plots/mlp_training_{shot_config}.png', dpi=dpi)\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(mlp, 'MLP', shot_config)\n",
    "        \n",
    "        # Create a custom predict function\n",
    "        def predict_fn(X):\n",
    "            X_reshaped = X.reshape(X.shape[0], -1)\n",
    "            X_scaled = scaler.transform(X_reshaped)\n",
    "            return mlp.predict(X_scaled, verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_model_imbalanced(mlp, X_test_2d, y_test, 'MLP', shot_config)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save the model\n",
    "        best_models[f\"MLP_{shot_config}\"] = {'model': mlp, 'scaler': scaler, 'predict_fn': predict_fn}\n",
    "        \n",
    "        # Clear TensorFlow session to free memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        return mlp, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in MLP training: {e}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        return None, None\n",
    "    \n",
    "# Function to train CNN model\n",
    "def train_cnn(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate CNN model\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    print(f\"\\nTraining CNN ({shot_config})...\")\n",
    "    \n",
    "    try:\n",
    "        # Reshape data for CNN (samples, timesteps, features)\n",
    "        if len(X_train.shape) == 2:\n",
    "            # Reshape to (samples, 300, 40) for CNN input\n",
    "            X_train_cnn = X_train.reshape(X_train.shape[0], 300, 40)\n",
    "            X_test_cnn = X_test.reshape(X_test.shape[0], 300, 40)\n",
    "        else:\n",
    "            X_train_cnn = X_train\n",
    "            X_test_cnn = X_test\n",
    "        \n",
    "        # Build CNN model with reduced complexity\n",
    "        cnn = Sequential([\n",
    "            Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=X_train_cnn.shape[1:]),  # Reduced filters\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Conv1D(filters=32, kernel_size=3, activation='relu'),  # Reduced filters\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Flatten(),\n",
    "            Dense(32, activation='relu'),  # Reduced neurons\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Set epochs based on shot configuration - reduced for all configs\n",
    "        if shot_config == \"10-shot\":\n",
    "            max_epochs = 15  # Reduced\n",
    "        elif shot_config == \"17-shot\":\n",
    "            max_epochs = 8\n",
    "        elif shot_config == \"5-shot\":\n",
    "            max_epochs = 10\n",
    "        else:\n",
    "            max_epochs = 25  # Reduced\n",
    "        \n",
    "        # Early stopping with specific monitoring based on configuration\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=5,  # Reduced patience\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Class weights for imbalanced data\n",
    "        class_weights = None\n",
    "        if shot_config == \"full\":\n",
    "            class_counts = Counter(y_train)\n",
    "            n_samples = len(y_train)\n",
    "            class_weights = {\n",
    "                label: n_samples / (len(class_counts) * count)\n",
    "                for label, count in class_counts.items()\n",
    "            }\n",
    "        \n",
    "        # Train model\n",
    "        batch_size = min(32, len(X_train) // 2) if len(X_train) > 1 else 1\n",
    "        history = cnn.fit(\n",
    "            X_train_cnn, y_train,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1 if len(X_train) > 10 else 0.0,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weights,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Plot training history with low DPI\n",
    "        dpi = 72\n",
    "        plt.figure(figsize=(12, 4), dpi=dpi)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'CNN Loss ({shot_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'CNN Accuracy ({shot_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/plots/cnn_training_{shot_config}.png', dpi=dpi)\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(cnn, 'CNN', shot_config)\n",
    "        \n",
    "        # Create a proper predict function that handles reshaping\n",
    "        def predict_fn(X):\n",
    "            if len(X.shape) == 2:\n",
    "                # Reshape to match the CNN input shape\n",
    "                X_reshaped = X.reshape(X.shape[0], 300, 40)\n",
    "            else:\n",
    "                X_reshaped = X\n",
    "            return cnn.predict(X_reshaped, verbose=0)\n",
    "        \n",
    "        # Use the custom predict function for evaluation\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, predict_function):\n",
    "                self.predict = predict_function\n",
    "        \n",
    "        model_wrapper = ModelWrapper(predict_fn)\n",
    "        \n",
    "        # Evaluate with the wrapper\n",
    "        results = evaluate_model_imbalanced(model_wrapper, X_test, y_test, 'CNN', shot_config)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save the model\n",
    "        best_models[f\"CNN_{shot_config}\"] = {'model': cnn, 'predict_fn': predict_fn}\n",
    "        \n",
    "        # Clear TensorFlow session to free memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        return cnn, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in CNN training: {e}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        return None, None\n",
    "    \n",
    "# Function to extract CNN features\n",
    "def extract_cnn_features(model, X, max_samples=1000):\n",
    "    \"\"\"Extract features from CNN for use with XGBoost\"\"\"\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    try:\n",
    "        # Limit the number of samples to process at once to avoid memory issues\n",
    "        if len(X) > max_samples:\n",
    "            X_subset = X[:max_samples]\n",
    "            print(f\"Warning: Processing only {max_samples} samples for feature extraction to save memory\")\n",
    "        else:\n",
    "            X_subset = X\n",
    "            \n",
    "        # Create a feature extractor model\n",
    "        feature_layer = -3  # Usually the layer before the final Dense layers\n",
    "        feature_extractor = tf.keras.Model(\n",
    "            inputs=model.inputs,\n",
    "            outputs=model.layers[feature_layer].output\n",
    "        )\n",
    "        \n",
    "        # Ensure X has right shape\n",
    "        if len(X_subset.shape) == 2:\n",
    "            # Reshape to match the CNN input shape\n",
    "            X_reshaped = X_subset.reshape(X_subset.shape[0], 300, 40)\n",
    "        else:\n",
    "            X_reshaped = X_subset\n",
    "        \n",
    "        # Extract features in batches to save memory\n",
    "        batch_size = 32\n",
    "        features = []\n",
    "        for i in range(0, len(X_reshaped), batch_size):\n",
    "            batch = X_reshaped[i:i+batch_size]\n",
    "            batch_features = feature_extractor.predict(batch, verbose=0)\n",
    "            features.append(batch_features)\n",
    "        \n",
    "        # Combine batches\n",
    "        features = np.concatenate(features, axis=0)\n",
    "        \n",
    "        # Reshape if needed\n",
    "        if len(features.shape) > 2:\n",
    "            features = features.reshape(features.shape[0], -1)\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting CNN features: {e}\")\n",
    "        return np.zeros((len(X), 10))  # Return dummy features on error\n",
    "\n",
    "# Function to train CNN+XGBoost model\n",
    "def train_cnn_xgboost(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate CNN+XGBoost model\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    import xgboost as xgb\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(f\"\\nTraining CNN+XGBoost ({shot_config})...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if we already have a trained CNN\n",
    "        cnn_key = f\"CNN_{shot_config}\"\n",
    "        if cnn_key in best_models:\n",
    "            cnn = best_models[cnn_key]['model']\n",
    "        else:\n",
    "            # Train a new CNN\n",
    "            cnn, _ = train_cnn(X_train, y_train, X_test, y_test, shot_config)\n",
    "        \n",
    "        # Extract CNN features - use a smaller subset if dataset is large\n",
    "        max_samples_train = min(500, len(X_train))\n",
    "        max_samples_test = min(200, len(X_test))\n",
    "        \n",
    "        X_train_subset = X_train[:max_samples_train]\n",
    "        y_train_subset = y_train[:max_samples_train]\n",
    "        X_test_subset = X_test[:max_samples_test]\n",
    "        \n",
    "        X_train_features = extract_cnn_features(cnn, X_train_subset)\n",
    "        X_test_features = extract_cnn_features(cnn, X_test_subset)\n",
    "        \n",
    "        # Calculate positive class weight for XGBoost\n",
    "        class_counts = Counter(y_train_subset)\n",
    "        scale_pos_weight = class_counts[0] / class_counts[1] if 1 in class_counts else 1.0\n",
    "        \n",
    "        # Train XGBoost on CNN features with reduced complexity\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=50,  # Reduced\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(X_train_features, y_train_subset)\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture({'cnn': cnn, 'xgb': xgb_model}, 'CNN_XGBoost', shot_config)\n",
    "        \n",
    "        # Create a custom predict function that combines CNN feature extraction with XGBoost\n",
    "        def predict_fn(X):\n",
    "            # Limit the number of samples for prediction to avoid memory issues\n",
    "            if len(X) > 200:\n",
    "                print(f\"Warning: Processing only 200 samples for prediction to save memory\")\n",
    "                X_subset = X[:200]\n",
    "                features = extract_cnn_features(cnn, X_subset)\n",
    "                probs = xgb_model.predict_proba(features)[:, 1].reshape(-1, 1)\n",
    "                # Pad with default predictions for the rest\n",
    "                pad_size = len(X) - 200\n",
    "                padding = np.ones((pad_size, 1)) * 0.5  # Default prediction\n",
    "                return np.vstack([probs, padding])\n",
    "            else:\n",
    "                features = extract_cnn_features(cnn, X)\n",
    "                return xgb_model.predict_proba(features)[:, 1].reshape(-1, 1)\n",
    "        \n",
    "        # Use a wrapper for evaluation\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, predict_function):\n",
    "                self.predict = predict_function\n",
    "        \n",
    "        model_wrapper = ModelWrapper(predict_fn)\n",
    "        \n",
    "        # Evaluate on a subset\n",
    "        results = evaluate_model_imbalanced(model_wrapper, X_test_subset, y_test[:max_samples_test], 'CNN_XGBoost', shot_config)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save the model\n",
    "        best_models[f\"CNN_XGBoost_{shot_config}\"] = {'cnn': cnn, 'xgb': xgb_model, 'predict_fn': predict_fn}\n",
    "        \n",
    "        # Clear TensorFlow session and release memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        return (cnn, xgb_model), results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in CNN+XGBoost training: {e}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        return None, None\n",
    "    \n",
    "# Function to train LSTM model\n",
    "def train_lstm(X_train, y_train, X_test, y_test, shot_config=\"full\"):\n",
    "    \"\"\"Train and evaluate LSTM model\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, LSTM as KerasLSTM\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    \n",
    "    print(f\"\\nTraining LSTM ({shot_config})...\")\n",
    "    \n",
    "    try:\n",
    "        # Reshape data for LSTM (samples, timesteps, features)\n",
    "        if len(X_train.shape) == 2:\n",
    "            # Reshape to match the expected LSTM input\n",
    "            X_train_lstm = X_train.reshape(X_train.shape[0], 300, 40)\n",
    "            X_test_lstm = X_test.reshape(X_test.shape[0], 300, 40)\n",
    "        else:\n",
    "            X_train_lstm = X_train\n",
    "            X_test_lstm = X_test\n",
    "        \n",
    "        # Build LSTM model with simplified architecture to reduce memory usage\n",
    "        lstm_model = Sequential([\n",
    "            KerasLSTM(32, return_sequences=True, input_shape=X_train_lstm.shape[1:], \n",
    "                    kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)),\n",
    "            Dropout(0.4),\n",
    "            KerasLSTM(16, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)),\n",
    "            Dropout(0.4),\n",
    "            Dense(8, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Set epochs and patience based on shot configuration - reduced for all configs\n",
    "        if shot_config == \"5-shot\":\n",
    "            max_epochs = 15  # Reduced\n",
    "            patience = 10     # Reduced\n",
    "        else:\n",
    "            max_epochs = 25   # Reduced\n",
    "            patience = 5      # Reduced\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='loss' if len(X_train) <= 20 else 'val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Add learning rate reduction to help with convergence\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        # Class weights for imbalanced data\n",
    "        class_weights = None\n",
    "        if shot_config == \"full\":\n",
    "            class_counts = Counter(y_train)\n",
    "            n_samples = len(y_train)\n",
    "            class_weights = {\n",
    "                label: n_samples / (len(class_counts) * count)\n",
    "                for label, count in class_counts.items()\n",
    "            }\n",
    "        \n",
    "        # Train model with smaller batch size\n",
    "        batch_size = min(16, len(X_train) // 2) if len(X_train) > 1 else 1  # Reduced batch size\n",
    "        history = lstm_model.fit(\n",
    "            X_train_lstm, y_train,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1 if len(X_train) > 10 else 0.0,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            class_weight=class_weights,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Plot training history with low DPI\n",
    "        dpi = 72\n",
    "        plt.figure(figsize=(12, 4), dpi=dpi)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'LSTM Loss ({shot_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'LSTM Accuracy ({shot_config})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/plots/lstm_training_{shot_config}.png', dpi=dpi)\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Save the model architecture\n",
    "        save_model_architecture(lstm_model, 'LSTM', shot_config)\n",
    "        \n",
    "        # Create a proper predict function\n",
    "        def predict_fn(X):\n",
    "            if len(X.shape) == 2:\n",
    "                # Reshape to match the LSTM input shape\n",
    "                X_reshaped = X.reshape(X.shape[0], 300, 40)\n",
    "            else:\n",
    "                X_reshaped = X\n",
    "            return lstm_model.predict(X_reshaped, verbose=0)\n",
    "        \n",
    "        # Use a wrapper for evaluation\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, predict_function):\n",
    "                self.predict = predict_function\n",
    "        \n",
    "        model_wrapper = ModelWrapper(predict_fn)\n",
    "        \n",
    "        # Evaluate on a subset of test data to save memory\n",
    "        max_test_samples = min(200, len(X_test))\n",
    "        X_test_subset = X_test[:max_test_samples]\n",
    "        y_test_subset = y_test[:max_test_samples]\n",
    "        \n",
    "        results = evaluate_model_imbalanced(model_wrapper, X_test_subset, y_test_subset, 'LSTM', shot_config)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save the model\n",
    "        best_models[f\"LSTM_{shot_config}\"] = {'model': lstm_model, 'predict_fn': predict_fn}\n",
    "        \n",
    "        # Clear TensorFlow session to free memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "        return lstm_model, results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM training: {e}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        return None, None\n",
    "    \n",
    "# Function to create results table\n",
    "def create_results_table():\n",
    "    \"\"\"Create and save a table of all model results with optimized memory usage\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Extract metrics from results\n",
    "    metrics_list = []\n",
    "    for result in all_results:\n",
    "        metrics = {\n",
    "            'model_name': result['model_name'],\n",
    "            'accuracy': result['accuracy'],\n",
    "            'precision': result['precision'],\n",
    "            'recall': result['recall'],\n",
    "            'f1_score': result['f1_score'],\n",
    "            'average_precision': result.get('average_precision', float('nan'))\n",
    "        }\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Sort by F1 score (descending)\n",
    "    results_df = results_df.sort_values('f1_score', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv('results/tables/model_performance.csv', index=False)\n",
    "    \n",
    "    # Create a more readable format for display\n",
    "    display_df = results_df.copy()\n",
    "    display_df = display_df.round(3)\n",
    "    \n",
    "    # Save as HTML for better visualization\n",
    "    display_df.to_html('results/tables/model_performance.html')\n",
    "    \n",
    "    # Create separate tables for each shot configuration with less processing\n",
    "    shot_configs = ['full', 'balanced'] + [f\"{shot}-shot\" for shot in [1, 3, 5, 10, 17]]\n",
    "    for config in shot_configs:\n",
    "        config_df = display_df[display_df['model_name'].str.endswith(config)]\n",
    "        if not config_df.empty:\n",
    "            config_df.to_csv(f'results/tables/model_performance_{config}.csv', index=False)\n",
    "    \n",
    "    # Plot performance comparison for top models only\n",
    "    dpi = 72\n",
    "    plt.figure(figsize=(14, 8), dpi=dpi)\n",
    "    \n",
    "    # Get top 10 models only\n",
    "    top_models = results_df.head(10)\n",
    "    \n",
    "    # Create simpler bar plot\n",
    "    sns.barplot(x='model_name', y='f1_score', data=top_models)\n",
    "    plt.title('F1 Score for Top 10 Models')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/plots/top_models_f1.png', dpi=dpi)\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def visualize_model(model_info, X_train, y_train, X_test, y_test, model_type, shot_config=\"full\"):\n",
    "    \"\"\"Create optimized visualizations for each model type\"\"\"\n",
    "    import gc\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "    \n",
    "    print(f\"\\nCreating visualizations for {model_type} ({shot_config})...\")\n",
    "    \n",
    "    # Create directory for visualizations if it doesn't exist\n",
    "    os.makedirs('results/plots/model_viz', exist_ok=True)\n",
    "    \n",
    "    # Use lower DPI to reduce memory usage\n",
    "    dpi = 72\n",
    "    \n",
    "    # Limit sample size for visualization\n",
    "    max_samples = min(200, len(X_test))\n",
    "    X_test_sample = X_test[:max_samples]\n",
    "    y_test_sample = y_test[:max_samples]\n",
    "    \n",
    "    # Reshape data if needed\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1) if len(X_train.shape) > 2 else X_train\n",
    "    X_test_2d = X_test_sample.reshape(X_test_sample.shape[0], -1) if len(X_test_sample.shape) > 2 else X_test_sample\n",
    "    \n",
    "    try:\n",
    "        # ROC Curve visualization for all models\n",
    "        if hasattr(model_info, 'predict_proba') or (isinstance(model_info, dict) and 'predict_fn' in model_info):\n",
    "            try:\n",
    "                # Get predictions\n",
    "                if isinstance(model_info, dict):\n",
    "                    if 'predict_fn' in model_info:\n",
    "                        # Use custom predict function\n",
    "                        y_pred_prob = model_info['predict_fn'](X_test_sample)\n",
    "                        if isinstance(y_pred_prob, np.ndarray) and y_pred_prob.ndim > 1:\n",
    "                            if y_pred_prob.shape[1] == 2:  # Binary classification with 2 columns\n",
    "                                y_pred_prob = y_pred_prob[:, 1]\n",
    "                            elif y_pred_prob.shape[1] == 1:  # Single column output\n",
    "                                y_pred_prob = y_pred_prob.flatten()\n",
    "                    elif 'model' in model_info:\n",
    "                        # Use model with potential scaler\n",
    "                        if 'scaler' in model_info:\n",
    "                            X_test_scaled = model_info['scaler'].transform(X_test_2d)\n",
    "                            if hasattr(model_info['model'], 'predict_proba'):\n",
    "                                y_pred_prob = model_info['model'].predict_proba(X_test_scaled)[:, 1]\n",
    "                            else:\n",
    "                                y_pred_prob = model_info['model'].predict(X_test_scaled).flatten()\n",
    "                        else:\n",
    "                            if hasattr(model_info['model'], 'predict_proba'):\n",
    "                                y_pred_prob = model_info['model'].predict_proba(X_test_2d)[:, 1]\n",
    "                            else:\n",
    "                                y_pred_prob = model_info['model'].predict(X_test_2d).flatten()\n",
    "                elif hasattr(model_info, 'predict_proba'):\n",
    "                    y_pred_prob = model_info.predict_proba(X_test_2d)[:, 1]\n",
    "                else:\n",
    "                    y_pred_prob = model_info.predict(X_test_2d).flatten()\n",
    "                \n",
    "                # Ensure y_pred_prob is of correct shape\n",
    "                if isinstance(y_pred_prob, np.ndarray) and y_pred_prob.ndim > 1:\n",
    "                    y_pred_prob = y_pred_prob.flatten()\n",
    "                \n",
    "                # Calculate ROC curve\n",
    "                fpr, tpr, _ = roc_curve(y_test_sample, y_pred_prob)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                # Plot ROC curve\n",
    "                plt.figure(figsize=(8, 6), dpi=dpi)\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'ROC Curve - {model_type} ({shot_config})')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'results/plots/model_viz/roc_{model_type}_{shot_config}.png', dpi=dpi)\n",
    "                plt.close('all')\n",
    "                \n",
    "                # Precision-Recall Curve\n",
    "                precision, recall, _ = precision_recall_curve(y_test_sample, y_pred_prob)\n",
    "                avg_precision = average_precision_score(y_test_sample, y_pred_prob)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6), dpi=dpi)\n",
    "                plt.plot(recall, precision, color='blue', lw=2, \n",
    "                        label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
    "                plt.xlabel('Recall')\n",
    "                plt.ylabel('Precision')\n",
    "                plt.title(f'Precision-Recall Curve - {model_type} ({shot_config})')\n",
    "                plt.legend(loc=\"lower left\")\n",
    "                plt.savefig(f'results/plots/model_viz/pr_curve_{model_type}_{shot_config}.png', dpi=dpi)\n",
    "                plt.close('all')\n",
    "                \n",
    "                # Force garbage collection\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating ROC or PR curve for {model_type}: {e}\")\n",
    "                plt.close('all')\n",
    "                gc.collect()\n",
    "        \n",
    "        # Create confusion matrix for all models\n",
    "        try:\n",
    "            # Get predictions\n",
    "            if isinstance(model_info, dict):\n",
    "                if 'predict_fn' in model_info:\n",
    "                    y_pred = np.round(model_info['predict_fn'](X_test_sample)).astype(int).flatten()\n",
    "                elif 'model' in model_info:\n",
    "                    if 'scaler' in model_info:\n",
    "                        X_test_scaled = model_info['scaler'].transform(X_test_2d)\n",
    "                        y_pred = np.round(model_info['model'].predict(X_test_scaled)).astype(int).flatten()\n",
    "                    else:\n",
    "                        if model_type in ['CNN', 'LSTM']:\n",
    "                            X_test_reshaped = X_test_sample.reshape(X_test_sample.shape[0], 300, 40)\n",
    "                            y_pred = np.round(model_info['model'].predict(X_test_reshaped, verbose=0)).astype(int).flatten()\n",
    "                        else:\n",
    "                            y_pred = np.round(model_info['model'].predict(X_test_2d)).astype(int).flatten()\n",
    "            else:\n",
    "                y_pred = np.round(model_info.predict(X_test_2d)).astype(int).flatten()\n",
    "            \n",
    "            # Create confusion matrix\n",
    "            from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "            cm = confusion_matrix(y_test_sample, y_pred)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(8, 6), dpi=dpi)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                        display_labels=['Field Pause', 'Filled Pause'])\n",
    "            disp.plot(cmap=plt.cm.Blues)\n",
    "            plt.title(f'Confusion Matrix - {model_type} ({shot_config})')\n",
    "            plt.savefig(f'results/plots/model_viz/cm_{model_type}_{shot_config}.png', dpi=dpi)\n",
    "            plt.close('all')\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating confusion matrix for {model_type}: {e}\")\n",
    "            plt.close('all')\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization for {model_type}: {e}\")\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "\n",
    "    return True\n",
    "\n",
    "def apply_xai_to_model(model_info, X_train, X_test, y_test, model_type, shot_config=\"full\"):\n",
    "    \"\"\"Apply explainable AI techniques to a model (optimized for memory usage)\"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    print(f\"\\nApplying XAI to {model_type} ({shot_config})...\")\n",
    "    \n",
    "    # Use a smaller subset of the data for XAI to save memory\n",
    "    max_samples = min(100, len(X_test))\n",
    "    X_test_sample = X_test[:max_samples]\n",
    "    y_test_sample = y_test[:max_samples]\n",
    "    \n",
    "    # Prepare data in appropriate format - ensure 2D arrays\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1) if len(X_train.shape) > 2 else X_train\n",
    "    X_test_2d = X_test_sample.reshape(X_test_sample.shape[0], -1) if len(X_test_sample.shape) > 2 else X_test_sample\n",
    "    \n",
    "    # Get predictions based on model type\n",
    "    try:\n",
    "        if isinstance(model_info, dict):\n",
    "            if 'predict_fn' in model_info:\n",
    "                # Use the model's custom predict function\n",
    "                y_pred_prob = model_info['predict_fn'](X_test_sample)\n",
    "                # Ensure y_pred is properly shaped\n",
    "                if isinstance(y_pred_prob, np.ndarray):\n",
    "                    if y_pred_prob.ndim > 1 and y_pred_prob.shape[1] == 1:\n",
    "                        y_pred_prob = y_pred_prob.flatten()\n",
    "                    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                else:\n",
    "                    # Handle case where predict_fn returns something unexpected\n",
    "                    y_pred = np.zeros(len(X_test_sample))\n",
    "                    print(f\"Warning: Unexpected predict_fn output for {model_type}\")\n",
    "            elif 'model' in model_info:\n",
    "                # Handle models with scalers\n",
    "                if 'scaler' in model_info:\n",
    "                    X_test_scaled = model_info['scaler'].transform(X_test_2d)\n",
    "                    if hasattr(model_info['model'], 'predict'):\n",
    "                        try:\n",
    "                            y_pred = model_info['model'].predict(X_test_scaled)\n",
    "                            if hasattr(y_pred, 'ndim') and y_pred.ndim > 1 and y_pred.shape[1] == 1:\n",
    "                                y_pred = y_pred.flatten()\n",
    "                            y_pred = np.round(y_pred).astype(int)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error predicting with {model_type}: {e}\")\n",
    "                            y_pred = np.zeros(len(X_test_sample))\n",
    "                    else:\n",
    "                        print(f\"Model {model_type} doesn't support predict method\")\n",
    "                        y_pred = np.zeros(len(X_test_sample))\n",
    "                else:\n",
    "                    # Handle models without scalers\n",
    "                    if model_type in ['CNN', 'LSTM', 'MLP']:\n",
    "                        # For neural networks, we need to reshape\n",
    "                        try:\n",
    "                            # For CNN/LSTM, reshape to expected dimensions\n",
    "                            if model_type in ['CNN', 'LSTM']:\n",
    "                                X_reshaped = X_test_sample.reshape(X_test_sample.shape[0], 300, 40)\n",
    "                                y_pred = model_info['model'].predict(X_reshaped, verbose=0)\n",
    "                            else:\n",
    "                                y_pred = model_info['model'].predict(X_test_2d, verbose=0)\n",
    "                            \n",
    "                            if hasattr(y_pred, 'ndim') and y_pred.ndim > 1 and y_pred.shape[1] == 1:\n",
    "                                y_pred = y_pred.flatten()\n",
    "                            y_pred = np.round(y_pred).astype(int)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error predicting with {model_type}: {e}\")\n",
    "                            y_pred = np.zeros(len(X_test_sample))\n",
    "                    else:\n",
    "                        # For other models\n",
    "                        try:\n",
    "                            y_pred = model_info['model'].predict(X_test_2d)\n",
    "                            if hasattr(y_pred, 'ndim') and y_pred.ndim > 1 and y_pred.shape[1] == 1:\n",
    "                                y_pred = y_pred.flatten()\n",
    "                            y_pred = np.round(y_pred).astype(int)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error predicting with {model_type}: {e}\")\n",
    "                            y_pred = np.zeros(len(X_test_sample))\n",
    "        else:\n",
    "            # Handle the case where model_info is the model itself\n",
    "            try:\n",
    "                y_pred = model_info.predict(X_test_2d)\n",
    "                if hasattr(y_pred, 'ndim') and y_pred.ndim > 1 and y_pred.shape[1] == 1:\n",
    "                    y_pred = y_pred.flatten()\n",
    "                y_pred = np.round(y_pred).astype(int)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting with {model_type}: {e}\")\n",
    "                y_pred = np.zeros(len(X_test_sample))\n",
    "        \n",
    "        # Ensure y_pred is 1D for comparison\n",
    "        if hasattr(y_pred, 'ndim') and y_pred.ndim > 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        \n",
    "        # Convert to numpy arrays if they aren't already\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_test_sample = np.array(y_test_sample)\n",
    "        \n",
    "        # Calculate and display model performance metrics\n",
    "        accuracy = accuracy_score(y_test_sample, y_pred)\n",
    "        precision = precision_score(y_test_sample, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test_sample, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test_sample, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"\\nPerformance metrics for {model_type} ({shot_config}):\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        cm = confusion_matrix(y_test_sample, y_pred)\n",
    "        plt.figure(figsize=(8, 6), dpi=72)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Field Pause', 'Filled Pause'],\n",
    "                    yticklabels=['Field Pause', 'Filled Pause'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {model_type} ({shot_config})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/plots/xai_cm_{model_type}_{shot_config}.png', dpi=72)\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        \n",
    "        # Apply XAI methods selectively based on model type to save memory\n",
    "        # Only apply LIME to traditional ML models\n",
    "        if model_type in ['kNN', 'RF_RFE', 'RF', 'SVM', 'WeightedSVM', 'BalancedRF', 'WeightedXGB', 'RUSBoost']:\n",
    "            try:\n",
    "                # Try to import LIME\n",
    "                import lime\n",
    "                import lime.lime_tabular\n",
    "                \n",
    "                # Find correctly predicted samples for explanation\n",
    "                correct_indices = np.where(y_pred == y_test_sample)[0]\n",
    "                \n",
    "                # If we have correctly predicted samples, use them for LIME\n",
    "                if len(correct_indices) > 0:\n",
    "                    # Select one sample from each class if possible\n",
    "                    sample_indices = []\n",
    "                    for cls in np.unique(y_test_sample):\n",
    "                        cls_correct = [i for i in correct_indices if y_test_sample[i] == cls]\n",
    "                        if cls_correct:\n",
    "                            sample_indices.append(cls_correct[0])\n",
    "                    \n",
    "                    # If no samples found, use the first correct index\n",
    "                    if not sample_indices:\n",
    "                        sample_indices = [correct_indices[0]]\n",
    "                    \n",
    "                    # Create feature names\n",
    "                    feature_names = [f'feature_{i}' for i in range(X_test_2d.shape[1])]\n",
    "                    \n",
    "                    # Create and save LIME explanations for selected samples\n",
    "                    for idx in sample_indices[:1]:  # Limit to 1 sample to save memory\n",
    "                        try:\n",
    "                            # Initialize LIME explainer\n",
    "                            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "                                X_train_2d[:min(500, len(X_train_2d))],  # Use subset of training data\n",
    "                                feature_names=feature_names,\n",
    "                                class_names=['Field Pause', 'Filled Pause'],\n",
    "                                mode='classification',\n",
    "                                random_state=42\n",
    "                            )\n",
    "                            \n",
    "                            # Create explanation function based on model type\n",
    "                            if isinstance(model_info, dict) and 'model' in model_info:\n",
    "                                if 'scaler' in model_info:\n",
    "                                    def predict_fn(x):\n",
    "                                        x_scaled = model_info['scaler'].transform(x)\n",
    "                                        return model_info['model'].predict_proba(x_scaled)\n",
    "                                else:\n",
    "                                    def predict_fn(x):\n",
    "                                        return model_info['model'].predict_proba(x)\n",
    "                            elif hasattr(model_info, 'predict_proba'):\n",
    "                                def predict_fn(x):\n",
    "                                    return model_info.predict_proba(x)\n",
    "                            else:\n",
    "                                print(f\"Model {model_type} doesn't support probability predictions for LIME\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Generate explanation with fewer features\n",
    "                            exp = explainer.explain_instance(\n",
    "                                X_test_2d[idx],\n",
    "                                predict_fn,\n",
    "                                num_features=8  # Reduced number of features\n",
    "                            )\n",
    "                            \n",
    "                            # Save explanation as image only (more memory efficient than HTML)\n",
    "                            class_name = 'field_pause' if y_test_sample[idx] == 0 else 'filled_pause'\n",
    "                            plt.figure(figsize=(10, 6), dpi=72)\n",
    "                            exp.as_pyplot_figure()\n",
    "                            plt.tight_layout()\n",
    "                            plt.savefig(f'results/plots/lime_{model_type}_{shot_config}_{class_name}.png', dpi=72)\n",
    "                            plt.close('all')\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating LIME explanation for sample {idx}: {e}\")\n",
    "                            plt.close('all')\n",
    "                    \n",
    "                    print(f\"LIME explanations created for {model_type}\")\n",
    "                else:\n",
    "                    print(f\"No correct predictions for LIME visualization in {model_type}\")\n",
    "                \n",
    "                # Force garbage collection\n",
    "                gc.collect()\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"LIME not installed. Skipping LIME explanations.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying LIME to {model_type}: {e}\")\n",
    "                plt.close('all')\n",
    "                gc.collect()\n",
    "        \n",
    "        # Return performance metrics\n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'shot_config': shot_config,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in XAI processing for {model_type}: {e}\")\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        return None\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"Main execution script to train and evaluate models for filled pause detection\"\"\"\n",
    "    import os\n",
    "    import gc\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Create necessary directories\n",
    "    os.makedirs('results/plots', exist_ok=True)\n",
    "    os.makedirs('results/tables', exist_ok=True)\n",
    "    os.makedirs('results/model_architecture', exist_ok=True)\n",
    "    os.makedirs('results/plots/model_viz', exist_ok=True)\n",
    "    \n",
    "    # Initialize global variables to store results\n",
    "    global all_results, best_models\n",
    "    all_results = []\n",
    "    best_models = {}\n",
    "    \n",
    "    print(\"Starting filled pause detection model evaluation with imbalanced data handling...\")\n",
    "    \n",
    "    # Starting with data loading (this section would be provided but let's stub it)\n",
    "    # Normally you would load X_train, y_train, X_test, y_test here\n",
    "    # For this example, we'll assume they're loaded already\n",
    "    \n",
    "    # Analyze data imbalance\n",
    "    print(\"\\nData distribution analysis:\")\n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"Class {label} ({'Field pause' if label == 0 else 'Filled pause'}): {count} samples ({count/total_samples*100:.2f}%)\")\n",
    "    \n",
    "    # Create balanced datasets\n",
    "    balanced_datasets = create_balanced_datasets(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Use a phased approach to training to avoid memory issues\n",
    "    \n",
    "    # Phase 1: Train specialized models for imbalanced data with full dataset\n",
    "    print(\"\\n\\n==== Phase 1: Training Specialized Models for Imbalanced Data ====\")\n",
    "    imbalanced_models = train_balanced_models(X_train, y_train, X_test, y_test, \"full\")\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    # Phase 2: Train regular models with the undersampled balanced dataset\n",
    "    print(\"\\n\\n==== Phase 2: Training with Undersampled Balanced Dataset ====\")\n",
    "    X_balanced, y_balanced = balanced_datasets['undersampled']\n",
    "    \n",
    "    # Machine Learning Models - less memory intensive\n",
    "    knn_model, _ = train_knn(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    rf_model, _ = train_rf_rfe(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    svm_model, _ = train_svm(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    # Deep Learning Models - more memory intensive, train them one by one\n",
    "    print(\"\\nTraining Deep Learning Models one by one to manage memory...\")\n",
    "    mlp_model, _ = train_mlp(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    gc.collect()\n",
    "    \n",
    "    cnn_model, _ = train_cnn(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    gc.collect()\n",
    "    \n",
    "    cnn_xgb_model, _ = train_cnn_xgboost(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    gc.collect()\n",
    "    \n",
    "    lstm_model, _ = train_lstm(X_balanced, y_balanced, X_test, y_test, \"balanced\")\n",
    "    gc.collect()\n",
    "    \n",
    "    # Phase 3: Train models with few-shot learning\n",
    "    print(\"\\n\\n==== Phase 3: Training with Few-Shot Learning ====\")\n",
    "    \n",
    "    # Group shot configs to train in batches\n",
    "    shot_configs = [config for config in balanced_datasets.keys() \n",
    "                   if config != 'undersampled' and int(config.split('-')[0]) >= 3]\n",
    "    \n",
    "    for shot_config in shot_configs:\n",
    "        X_few, y_few = balanced_datasets[shot_config]\n",
    "        print(f\"\\n--- Training with {shot_config} Learning ---\")\n",
    "        \n",
    "        # Traditional ML models\n",
    "        train_knn(X_few, y_few, X_test, y_test, shot_config)\n",
    "        train_rf_rfe(X_few, y_few, X_test, y_test, shot_config)\n",
    "        train_svm(X_few, y_few, X_test, y_test, shot_config)\n",
    "        gc.collect()\n",
    "        \n",
    "        # Only train complex models with larger shots\n",
    "        if int(shot_config.split('-')[0]) >= 5:\n",
    "            # Train these one by one to manage memory\n",
    "            train_balanced_models(X_few, y_few, X_test, y_test, shot_config)\n",
    "            gc.collect()\n",
    "            \n",
    "            if int(shot_config.split('-')[0]) >= 10:  # Skip the smallest datasets for deep learning\n",
    "                train_mlp(X_few, y_few, X_test, y_test, shot_config)\n",
    "                gc.collect()\n",
    "                \n",
    "                train_cnn(X_few, y_few, X_test, y_test, shot_config)\n",
    "                gc.collect()\n",
    "                \n",
    "                train_lstm(X_few, y_few, X_test, y_test, shot_config)\n",
    "                gc.collect()\n",
    "    \n",
    "    # Phase 4: Create results table and find best model\n",
    "    print(\"\\n\\n==== Phase 4: Analyzing Results ====\")\n",
    "    results_df = create_results_table()\n",
    "    \n",
    "    # Identify best model based on F1 score\n",
    "    best_result = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "    best_model_name = best_result['model_name']\n",
    "    print(f\"\\nBest model: {best_model_name} with F1 score: {best_result['f1_score']:.4f}\")\n",
    "    \n",
    "    # Phase 5: Apply XAI to top models only\n",
    "    print(\"\\n\\n==== Phase 5: Applying XAI to Top Models Only ====\")\n",
    "    \n",
    "    # Only process top 3 models for XAI to save memory\n",
    "    top_models = results_df.nlargest(3, 'f1_score')\n",
    "    xai_results = []\n",
    "    \n",
    "    for _, row in top_models.iterrows():\n",
    "        model_key = row['model_name']\n",
    "        model_type, shot_config = model_key.split('_', 1)\n",
    "        \n",
    "        if model_key in best_models:\n",
    "            model_info = best_models[model_key]\n",
    "            \n",
    "            # Apply visualizations\n",
    "            visualize_model(model_info, X_train, y_train, X_test, y_test, model_type, shot_config)\n",
    "            gc.collect()\n",
    "            \n",
    "            # Apply XAI with sample limit\n",
    "            result = apply_xai_to_model(model_info, X_train, X_test, y_test, model_type, shot_config)\n",
    "            if result:\n",
    "                xai_results.append(result)\n",
    "            gc.collect()\n",
    "    \n",
    "    # Create a summary of XAI results\n",
    "    if xai_results:\n",
    "        xai_df = pd.DataFrame(xai_results)\n",
    "        \n",
    "        # Save XAI results\n",
    "        xai_df.to_csv('results/tables/xai_performance.csv', index=False)\n",
    "        \n",
    "        # Plot XAI comparison for top models only\n",
    "        plt.figure(figsize=(10, 6), dpi=72)\n",
    "        sns.barplot(x='model_type', y='f1_score', data=xai_df)\n",
    "        plt.title('F1 Score Comparison for Top Models')\n",
    "        plt.xlabel('Model Type')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/plots/xai_comparison_top.png', dpi=72)\n",
    "        plt.close('all')\n",
    "        \n",
    "        print(\"\\nXAI analysis complete. Results saved to results/tables/xai_performance.csv\")\n",
    "        print(\"XAI visualizations saved to results/plots/ directory\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    gc.collect()\n",
    "    print(\"\\nAll processing complete!\")\n",
    "\n",
    "# Run the main function if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa620aa-e9a6-421f-820c-b9766d987985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_filled_pause)",
   "language": "python",
   "name": "venv_filled_pause"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
