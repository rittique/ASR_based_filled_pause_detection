{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dc1caa-b486-45c2-a221-0fab404f4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping segment 166 due to insufficient length: 0 samples\n",
      "Skipping segment 167 due to insufficient length: 0 samples\n",
      "Skipping segment 298 due to insufficient length: 677 samples\n",
      "Skipping segment 304 due to insufficient length: 0 samples\n",
      "Skipping segment 401 due to insufficient length: 0 samples\n",
      "Skipping segment 466 due to insufficient length: 0 samples\n",
      "Skipping segment 467 due to insufficient length: 0 samples\n",
      "Skipping segment 468 due to insufficient length: 0 samples\n",
      "Skipping segment 469 due to insufficient length: 0 samples\n",
      "Skipping segment 470 due to insufficient length: 0 samples\n",
      "Skipping segment 471 due to insufficient length: 0 samples\n",
      "Skipping segment 472 due to insufficient length: 0 samples\n",
      "Skipping segment 473 due to insufficient length: 0 samples\n",
      "Skipping segment 474 due to insufficient length: 0 samples\n",
      "Skipping segment 475 due to insufficient length: 0 samples\n",
      "Skipping segment 476 due to insufficient length: 0 samples\n",
      "Skipping segment 477 due to insufficient length: 0 samples\n",
      "Skipping segment 478 due to insufficient length: 0 samples\n",
      "Skipping segment 479 due to insufficient length: 0 samples\n",
      "Skipping segment 480 due to insufficient length: 0 samples\n",
      "Skipping segment 481 due to insufficient length: 0 samples\n",
      "Skipping segment 482 due to insufficient length: 0 samples\n",
      "Skipping segment 483 due to insufficient length: 0 samples\n",
      "Skipping segment 484 due to insufficient length: 0 samples\n",
      "Skipping segment 485 due to insufficient length: 0 samples\n",
      "Skipping segment 486 due to insufficient length: 0 samples\n",
      "Skipping segment 487 due to insufficient length: 0 samples\n",
      "Skipping segment 488 due to insufficient length: 0 samples\n",
      "Skipping segment 489 due to insufficient length: 0 samples\n",
      "Skipping segment 490 due to insufficient length: 0 samples\n",
      "Skipping segment 491 due to insufficient length: 0 samples\n",
      "Skipping segment 525 due to insufficient length: 0 samples\n",
      "Skipping segment 526 due to insufficient length: 0 samples\n",
      "Skipping segment 527 due to insufficient length: 0 samples\n",
      "Skipping segment 528 due to insufficient length: 0 samples\n",
      "Skipping segment 529 due to insufficient length: 0 samples\n",
      "Skipping segment 530 due to insufficient length: 0 samples\n",
      "Skipping segment 531 due to insufficient length: 0 samples\n",
      "Skipping segment 532 due to insufficient length: 0 samples\n",
      "Skipping segment 533 due to insufficient length: 0 samples\n",
      "Skipping segment 534 due to insufficient length: 0 samples\n",
      "Skipping segment 535 due to insufficient length: 0 samples\n",
      "Skipping segment 536 due to insufficient length: 0 samples\n",
      "Skipping segment 537 due to insufficient length: 0 samples\n",
      "Skipping segment 538 due to insufficient length: 0 samples\n",
      "Skipping segment 539 due to insufficient length: 0 samples\n",
      "Skipping segment 540 due to insufficient length: 0 samples\n",
      "Skipping segment 541 due to insufficient length: 0 samples\n",
      "Skipping segment 542 due to insufficient length: 0 samples\n",
      "Skipping segment 543 due to insufficient length: 0 samples\n",
      "Skipping segment 544 due to insufficient length: 0 samples\n",
      "Skipping segment 654 due to insufficient length: 0 samples\n",
      "Skipping segment 655 due to insufficient length: 0 samples\n",
      "Skipping segment 656 due to insufficient length: 0 samples\n",
      "Skipping segment 657 due to insufficient length: 0 samples\n",
      "Skipping segment 658 due to insufficient length: 0 samples\n",
      "Skipping segment 659 due to insufficient length: 0 samples\n",
      "Skipping segment 660 due to insufficient length: 0 samples\n",
      "Skipping segment 661 due to insufficient length: 0 samples\n",
      "Skipping segment 662 due to insufficient length: 0 samples\n",
      "Skipping segment 663 due to insufficient length: 0 samples\n",
      "Skipping segment 664 due to insufficient length: 0 samples\n",
      "Skipping segment 665 due to insufficient length: 0 samples\n",
      "Skipping segment 666 due to insufficient length: 0 samples\n",
      "Skipping segment 667 due to insufficient length: 0 samples\n",
      "Skipping segment 714 due to insufficient length: 0 samples\n",
      "Skipping segment 715 due to insufficient length: 0 samples\n",
      "Skipping segment 728 due to insufficient length: 0 samples\n",
      "Skipping segment 1140 due to insufficient length: 0 samples\n",
      "Skipping segment 1141 due to insufficient length: 0 samples\n",
      "Skipping segment 1142 due to insufficient length: 0 samples\n",
      "Skipping segment 1143 due to insufficient length: 0 samples\n",
      "Skipping segment 1199 due to insufficient length: 0 samples\n",
      "Skipping segment 1200 due to insufficient length: 0 samples\n",
      "Skipping segment 1201 due to insufficient length: 0 samples\n",
      "Skipping segment 1202 due to insufficient length: 0 samples\n",
      "Skipping segment 1203 due to insufficient length: 0 samples\n",
      "Skipping segment 1213 due to insufficient length: 0 samples\n",
      "Skipping segment 1214 due to insufficient length: 0 samples\n",
      "Skipping segment 1215 due to insufficient length: 0 samples\n",
      "Skipping segment 1216 due to insufficient length: 0 samples\n",
      "Skipping segment 1258 due to insufficient length: 0 samples\n",
      "Skipping segment 1259 due to insufficient length: 0 samples\n",
      "Skipping segment 1260 due to insufficient length: 0 samples\n",
      "Skipping segment 1261 due to insufficient length: 0 samples\n",
      "Skipping segment 1262 due to insufficient length: 0 samples\n",
      "Skipping segment 1263 due to insufficient length: 0 samples\n",
      "Skipping segment 1264 due to insufficient length: 0 samples\n",
      "Skipping segment 1265 due to insufficient length: 0 samples\n",
      "Skipping segment 1266 due to insufficient length: 0 samples\n",
      "Skipping segment 1267 due to insufficient length: 0 samples\n",
      "Skipping segment 1268 due to insufficient length: 0 samples\n",
      "Skipping segment 1269 due to insufficient length: 0 samples\n",
      "Skipping segment 1317 due to insufficient length: 0 samples\n",
      "Skipping segment 1318 due to insufficient length: 0 samples\n",
      "Skipping segment 1319 due to insufficient length: 0 samples\n",
      "Skipping segment 1320 due to insufficient length: 0 samples\n",
      "Skipping segment 1321 due to insufficient length: 0 samples\n",
      "Skipping segment 1322 due to insufficient length: 0 samples\n",
      "Skipping segment 1323 due to insufficient length: 0 samples\n",
      "Skipping segment 1324 due to insufficient length: 0 samples\n",
      "Skipping segment 1325 due to insufficient length: 0 samples\n",
      "Skipping segment 1326 due to insufficient length: 0 samples\n",
      "Skipping segment 1327 due to insufficient length: 0 samples\n",
      "Skipping segment 1417 due to insufficient length: 0 samples\n",
      "Skipping segment 1418 due to insufficient length: 0 samples\n",
      "Skipping segment 1419 due to insufficient length: 0 samples\n",
      "Skipping segment 1420 due to insufficient length: 0 samples\n",
      "Skipping segment 1421 due to insufficient length: 0 samples\n",
      "Skipping segment 1422 due to insufficient length: 0 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Paths to JSON and audio files\n",
    "json_folder = 'JSON'\n",
    "audio_folder = 'cleaned_wav_files'\n",
    "\n",
    "# Match audio and JSON files\n",
    "json_files = {os.path.splitext(f)[0]: os.path.join(json_folder, f) for f in os.listdir(json_folder) if f.endswith('.json')}\n",
    "audio_files = {os.path.splitext(f)[0]: os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(('.wav', '.m4a', '.mp3'))}\n",
    "\n",
    "matched_files = {name: (json_files[name], audio_files[name]) for name in json_files if name in audio_files}\n",
    "\n",
    "# Function to load annotations from a JSON file\n",
    "def load_annotations(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    annotations = []\n",
    "    for item in data[0]['annotations'][0]['result']:\n",
    "        if item['type'] == 'labels':\n",
    "            start = item['value']['start']\n",
    "            end = item['value']['end']\n",
    "            label = item['value']['labels'][0]\n",
    "            annotations.append((start, end, label))\n",
    "    return annotations\n",
    "\n",
    "# Function to extract audio segments based on annotations\n",
    "def extract_audio_segments(audio_file, annotations, sr=16000):\n",
    "    y, _ = librosa.load(audio_file, sr=sr)\n",
    "    segments = []\n",
    "    for start, end, label in annotations:\n",
    "        segment = y[int(start * sr):int(end * sr)]\n",
    "        segments.append((segment, label))\n",
    "    return segments\n",
    "\n",
    "dataset = []\n",
    "files_loaded = []\n",
    "\n",
    "for name, (json_path, audio_path) in matched_files.items():\n",
    "    if len(load_annotations(json_path)) != 0:\n",
    "        files_loaded.append(json_path)\n",
    "        annotations = load_annotations(json_path)\n",
    "        audio_segments = extract_audio_segments(audio_path, annotations)\n",
    "        dataset.extend(audio_segments)\n",
    "\n",
    "# Function to extract features (MFCC) from audio segments\n",
    "def extract_features(segments, n_mfcc=40, max_length=300):\n",
    "    features, labels = [], []\n",
    "    for i, (segment, label) in enumerate(segments):\n",
    "        try:\n",
    "            # Check if the segment is too short for FFT processing\n",
    "            if len(segment) < 2048:  # Minimum required length for n_fft=2048\n",
    "                print(f\"Skipping segment {i} due to insufficient length: {len(segment)} samples\")\n",
    "                continue\n",
    "\n",
    "            # Dynamically adjust n_fft based on signal length\n",
    "            n_fft = min(2048, len(segment))  # Use the smaller of 2048 or segment length\n",
    "\n",
    "            # Extract MFCC features with adjusted n_fft\n",
    "            mfcc = librosa.feature.mfcc(y=segment, sr=16000, n_mfcc=n_mfcc, n_fft=n_fft)\n",
    "\n",
    "            # Handle variable lengths (pad if short, truncate if long)\n",
    "            if mfcc.shape[1] < max_length:\n",
    "                padded_mfcc = np.pad(mfcc, ((0, 0), (0, max_length - mfcc.shape[1])), mode='constant')\n",
    "            else:\n",
    "                padded_mfcc = mfcc[:, :max_length]\n",
    "\n",
    "            # Append features and labels\n",
    "            features.append(padded_mfcc.T)\n",
    "            labels.append(0 if label == 'Field pause' else 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {i}: {e}\")\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Extract features and labels from the dataset\n",
    "X, y = extract_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2c1577-b3d8-4370-9033-b2e6e836304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Class 0: 21 samples\n",
      "Class 1: 1292 samples\n",
      "\n",
      "Balancing with smoteenn:\n",
      "Class 0: 1292 samples\n",
      "Class 1: 379 samples\n",
      "Balanced feature shape: (1671, 300, 40)\n",
      "\n",
      "Balancing with smote:\n",
      "Class 0: 1292 samples\n",
      "Class 1: 1292 samples\n",
      "Balanced feature shape: (2584, 300, 40)\n",
      "\n",
      "Balancing with random:\n",
      "Class 0: 21 samples\n",
      "Class 1: 21 samples\n",
      "Balanced feature shape: (42, 300, 40)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def balance_dataset(X, y, balance_strategy='smoteenn', random_state=42):\n",
    "    \"\"\"\n",
    "    Balance the dataset using various strategies.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array\n",
    "        Feature array of shape (n_samples, sequence_length, n_features)\n",
    "    y : numpy array\n",
    "        Labels array of shape (n_samples,)\n",
    "    balance_strategy : str\n",
    "        Strategy to use for balancing. Options: 'smoteenn', 'smote', 'random'\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_balanced : numpy array\n",
    "        Balanced feature array\n",
    "    y_balanced : numpy array\n",
    "        Balanced labels array\n",
    "    \"\"\"\n",
    "    # Reshape the 3D array to 2D for SMOTE\n",
    "    original_shape = X.shape\n",
    "    X_reshaped = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_reshaped)\n",
    "    \n",
    "    # Apply balancing strategy\n",
    "    if balance_strategy == 'smoteenn':\n",
    "        # SMOTEENN combines SMOTE with Edited Nearest Neighbors\n",
    "        balancer = SMOTEENN(random_state=random_state)\n",
    "        X_balanced, y_balanced = balancer.fit_resample(X_scaled, y)\n",
    "    \n",
    "    elif balance_strategy == 'smote':\n",
    "        # Only SMOTE for oversampling\n",
    "        balancer = SMOTE(random_state=random_state)\n",
    "        X_balanced, y_balanced = balancer.fit_resample(X_scaled, y)\n",
    "    \n",
    "    elif balance_strategy == 'random':\n",
    "        # Random under-sampling of majority class and over-sampling of minority class\n",
    "        over_sampler = RandomUnderSampler(random_state=random_state)\n",
    "        X_balanced, y_balanced = over_sampler.fit_resample(X_scaled, y)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid balance_strategy. Choose 'smoteenn', 'smote', or 'random'\")\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_balanced = X_balanced.reshape(-1, original_shape[1], original_shape[2])\n",
    "    \n",
    "    # Inverse transform to get back to original scale\n",
    "    X_balanced_reshaped = X_balanced.reshape(X_balanced.shape[0], -1)\n",
    "    X_balanced_scaled = scaler.inverse_transform(X_balanced_reshaped)\n",
    "    X_balanced = X_balanced_scaled.reshape(X_balanced.shape)\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Function to display class distribution\n",
    "def display_class_distribution(y):\n",
    "    \"\"\"Display the distribution of classes in the dataset.\"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"Class {label}: {count} samples\")\n",
    "\n",
    "# Balance the dataset\n",
    "print(\"Original class distribution:\")\n",
    "display_class_distribution(y)\n",
    "\n",
    "# Try different balancing strategies\n",
    "strategies = ['smoteenn', 'smote', 'random']\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nBalancing with {strategy}:\")\n",
    "    X_balanced, y_balanced = balance_dataset(X, y, balance_strategy=strategy)\n",
    "    display_class_distribution(y_balanced)\n",
    "    print(f\"Balanced feature shape: {X_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe6bdaa-6856-4cd0-ad62-5de61bb59dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SMOTEENN strategy:\n",
      "\n",
      "Evaluating fold 1/5\n",
      "11/11 [==============================] - 1s 35ms/step\n",
      "11/11 [==============================] - 0s 35ms/step\n",
      "\n",
      "Evaluating fold 2/5\n",
      "11/11 [==============================] - 1s 36ms/step\n",
      "11/11 [==============================] - 0s 35ms/step\n",
      "\n",
      "Evaluating fold 3/5\n",
      "11/11 [==============================] - 1s 35ms/step\n",
      "11/11 [==============================] - 0s 35ms/step\n",
      "\n",
      "Evaluating fold 4/5\n",
      "11/11 [==============================] - 1s 35ms/step\n",
      "11/11 [==============================] - 0s 35ms/step\n",
      "\n",
      "Evaluating fold 5/5\n",
      "11/11 [==============================] - 1s 36ms/step\n",
      "11/11 [==============================] - 0s 35ms/step\n",
      "\n",
      "Evaluating SMOTE strategy:\n",
      "\n",
      "Evaluating fold 1/5\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "\n",
      "Evaluating fold 2/5\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "\n",
      "Evaluating fold 3/5\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "\n",
      "Evaluating fold 4/5\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "17/17 [==============================] - 1s 39ms/step\n",
      "\n",
      "Evaluating fold 5/5\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "\n",
      "Evaluating Random strategy:\n",
      "\n",
      "Evaluating fold 1/5\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\n",
      "Evaluating fold 2/5\n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "\n",
      "Evaluating fold 3/5\n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "Evaluating fold 4/5\n",
      "WARNING:tensorflow:5 out of the last 41 calls to <function Model.make_predict_function.<locals>.predict_function at 0x394b4ae80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "Evaluating fold 5/5\n",
      "WARNING:tensorflow:6 out of the last 43 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3596d7ec0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "Dataset Comparison Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "SMOTEENN Strategy:\n",
      "ACCURACY  : 0.8509 (±0.0508)\n",
      "PRECISION : 0.9938 (±0.0125)\n",
      "RECALL    : 0.3480 (±0.2342)\n",
      "F1        : 0.4773 (±0.2047)\n",
      "AUC_ROC   : 0.7377 (±0.1274)\n",
      "\n",
      "SMOTE Strategy:\n",
      "ACCURACY  : 0.8498 (±0.1669)\n",
      "PRECISION : 0.9947 (±0.0107)\n",
      "RECALL    : 0.7052 (±0.3379)\n",
      "F1        : 0.7720 (±0.2628)\n",
      "AUC_ROC   : 0.8758 (±0.1493)\n",
      "\n",
      "Random Strategy:\n",
      "ACCURACY  : 0.5000 (±0.0865)\n",
      "PRECISION : 0.4222 (±0.3875)\n",
      "RECALL    : 0.3300 (±0.3682)\n",
      "F1        : 0.3031 (±0.2567)\n",
      "AUC_ROC   : 0.4363 (±0.1245)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rittique/Python_Projects/Filled_Pause_detection/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_baseline_model(input_shape):\n",
    "    \"\"\"Create a simple LSTM model for evaluation.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "        LSTM(32),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_dataset(X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Evaluate dataset quality using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array\n",
    "        Feature array of shape (n_samples, sequence_length, n_features)\n",
    "    y : numpy array\n",
    "        Labels array\n",
    "    n_splits : int\n",
    "        Number of cross-validation splits\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics storage\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'auc_roc': [],\n",
    "        'class_distribution': []\n",
    "    }\n",
    "    \n",
    "    # Create cross-validation splits\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nEvaluating fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_baseline_model((X.shape[1], X.shape[2]))\n",
    "        \n",
    "        # Early stopping to prevent overfitting\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "        y_pred_prob = model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_val, y_pred))\n",
    "        metrics['recall'].append(recall_score(y_val, y_pred))\n",
    "        metrics['f1'].append(f1_score(y_val, y_pred))\n",
    "        metrics['auc_roc'].append(roc_auc_score(y_val, y_pred_prob))\n",
    "        \n",
    "        # Calculate class distribution\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        metrics['class_distribution'].append(dict(zip(unique, counts)))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compare_datasets(datasets_dict):\n",
    "    \"\"\"\n",
    "    Compare multiple balanced datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets_dict : dict\n",
    "        Dictionary containing datasets with their balancing strategies\n",
    "        Format: {'strategy_name': (X_balanced, y_balanced)}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Comparison results for each strategy\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for strategy_name, (X_balanced, y_balanced) in datasets_dict.items():\n",
    "        print(f\"\\nEvaluating {strategy_name} strategy:\")\n",
    "        results[strategy_name] = evaluate_dataset(X_balanced, y_balanced)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_comparison_results(results):\n",
    "    \"\"\"Print formatted comparison results.\"\"\"\n",
    "    print(\"\\nDataset Comparison Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']\n",
    "    \n",
    "    for strategy, metrics_dict in results.items():\n",
    "        print(f\"\\n{strategy} Strategy:\")\n",
    "        for metric in metrics:\n",
    "            values = metrics_dict[metric]\n",
    "            mean_value = np.mean(values)\n",
    "            std_value = np.std(values)\n",
    "            print(f\"{metric.upper():10}: {mean_value:.4f} (±{std_value:.4f})\")\n",
    "\n",
    "# Create dictionary of balanced datasets\n",
    "balanced_datasets = {\n",
    "    'SMOTEENN': balance_dataset(X, y, 'smoteenn'),\n",
    "    'SMOTE': balance_dataset(X, y, 'smote'),\n",
    "    'Random': balance_dataset(X, y, 'random')\n",
    "}\n",
    "\n",
    "# Compare datasets\n",
    "results = compare_datasets(balanced_datasets)\n",
    "\n",
    "# Print results\n",
    "print_comparison_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0a908-e9da-456d-ad93-4854c02fbf32",
   "metadata": {},
   "source": [
    "### Decision\n",
    "As the problem requires a better Recall and F1 score thus the more balanced result from SMOTE will be selected. Moreover, SMOTE provides the best AUC_ROC score, indicating the generalisation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a6322f-1a08-4362-be03-816e93975a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balancing with smote:\n",
      "Class 0: 1292 samples\n",
      "Class 1: 1292 samples\n",
      "Balanced feature shape: (2584, 300, 40)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBalancing with smote:\")\n",
    "X_balanced, y_balanced = balance_dataset(X, y, balance_strategy=\"smote\")\n",
    "display_class_distribution(y_balanced)\n",
    "print(f\"Balanced feature shape: {X_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7792d-d1dc-4e03-9657-252a01bd52fd",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b5b47e-882d-43e0-9dde-dcead9acf020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1808, 300, 40), (1808,)\n",
      "Validation set shape: (388, 300, 40), (388,)\n",
      "Test set shape: (388, 300, 40), (388,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split the data into training and temp (test + validation)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# Now, split the temp set equally into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Display the shape of the datasets\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"train.csv\", index=False)\n",
    "val_data.to_csv(\"val.csv\", index=False)\n",
    "test_data.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"Datasets saved as CSV files successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72e4cd-b3ee-4cef-bdb3-533df501e9dd",
   "metadata": {},
   "source": [
    "## Model Selection:\n",
    "\n",
    "- Statistical Methods\n",
    "  - Gaussian Mixture Models (GMM)\n",
    "  - Hidden Markov Models (HMM)\n",
    "- Machine Learning Models\n",
    "  - k-Nearest Neighbors (k-NN)\n",
    "  - Random Forest (RF) with Recursive Feature Elimination (RFE)\n",
    "  - Support Vector Machines (SVM)\n",
    "- Deep Learning Models\n",
    "  - Multilayer Perceptron (MLP)\n",
    "  - Artificial Neural Networks (ANN)\n",
    "  - Convolutional Neural Networks (CNN)\n",
    "  - Convolutional Neural Networks (CNN) + XGBoost\n",
    "  - Recurrent Neural Networks (RNNs)\n",
    "  - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23714247-c009-49c4-9af3-4e6c7959ecba",
   "metadata": {},
   "source": [
    "## Statistical Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671cafe-27e4-440a-abab-f41868aca95e",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3415134-f729-4667-a1d0-854b97024e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67       194\n",
      "           1       0.67      0.58      0.62       194\n",
      "\n",
      "    accuracy                           0.65       388\n",
      "   macro avg       0.65      0.65      0.65       388\n",
      "weighted avg       0.65      0.65      0.65       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Flatten for GMM\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
    "gmm.fit(X_train_flat)\n",
    "\n",
    "y_pred = gmm.predict(X_test_flat)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde0db9-b913-466d-a4c1-3209dd72d8b9",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bece57a-7b2b-42a9-b543-6a798e7b9670",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3636c33b-e35b-40e2-a830-585a64736e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.90       194\n",
      "           1       1.00      0.79      0.88       194\n",
      "\n",
      "    accuracy                           0.89       388\n",
      "   macro avg       0.91      0.89      0.89       388\n",
      "weighted avg       0.91      0.89      0.89       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_flat, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test_flat)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a26e3-9e60-4903-b058-c6566115a82a",
   "metadata": {},
   "source": [
    "### Random Forest with Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6340876f-6f91-4b99-bc88-8cda071e57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Selection Progress:   0%|              | 0/12000 [00:00<?, ? features/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2000 features.\n",
      "Fitting estimator with 1500 features.\n",
      "Fitting estimator with 1000 features.\n",
      "Fitting estimator with 500 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Selection Progress:   0%|              | 0/12000 [00:11<?, ? features/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       194\n",
      "           1       1.00      0.98      0.99       194\n",
      "\n",
      "    accuracy                           0.99       388\n",
      "   macro avg       0.99      0.99      0.99       388\n",
      "weighted avg       0.99      0.99      0.99       388\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RFETqdm(RFE):\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Wrap RFE’s fit method with a tqdm progress bar.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        pbar = tqdm(total=n_features, desc=\"Feature Selection Progress\", unit=\" features\")\n",
    "\n",
    "        def callback(estimator, features_to_select):\n",
    "            pbar.update(n_features - features_to_select)\n",
    "\n",
    "        self.step = 500  # Speed up by removing 500 features at a time\n",
    "        self.verbose = 1\n",
    "        super().fit(X, y)\n",
    "        pbar.close()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)  # Reduce trees for faster RFE\n",
    "rfe = RFETqdm(estimator=rf, n_features_to_select=20, step=500)\n",
    "rfe.fit(X_train_flat, y_train)\n",
    "\n",
    "y_pred = rfe.predict(X_test_flat)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f69e07-6c22-4dd8-8c80-ae44954a171a",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f678d8-e887-4b83-ae72-2d04c6d08206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       194\n",
      "           1       1.00      0.95      0.97       194\n",
      "\n",
      "    accuracy                           0.97       388\n",
      "   macro avg       0.98      0.97      0.97       388\n",
      "weighted avg       0.98      0.97      0.97       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm.fit(X_train_flat, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test_flat)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c48c4b-cfac-45ec-8753-65efbbd68491",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722f416-781d-44b0-9808-84d7470584d5",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f551d876-63be-4ede-9a51-a75b126deb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 1s 5ms/step - loss: 3.7057 - accuracy: 0.8507 - val_loss: 0.0418 - val_accuracy: 0.9974\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9856 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9934 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 7.7552e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.9939 - val_loss: 0.0028 - val_accuracy: 0.9974\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9895 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9945 - val_loss: 5.0606e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9889 - val_loss: 8.4442e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9906 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9917 - val_loss: 6.6856e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x34aca9e90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "mlp_model = Sequential([\n",
    "    Flatten(input_shape=(300, 40)),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),  # Reduced dropout rate\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mlp_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b943c-063a-4236-8a09-d01f4b1417a0",
   "metadata": {},
   "source": [
    "### Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120f2f45-4ab1-425b-b21d-017e09acce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.2048 - accuracy: 0.8280 - val_loss: 9.2391e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.2849 - accuracy: 0.9790 - val_loss: 4.2563e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9912 - val_loss: 1.6449e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9934 - val_loss: 4.9691e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0862 - accuracy: 0.9934 - val_loss: 6.3085e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.9945 - val_loss: 2.2322e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9950 - val_loss: 0.0137 - val_accuracy: 0.9974\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0754 - accuracy: 0.9923 - val_loss: 5.0074e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9917 - val_loss: 4.8016e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1891 - accuracy: 0.9856 - val_loss: 0.0656 - val_accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1575d9590>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "ann_model = Sequential([\n",
    "    Flatten(input_shape=(300, 40)),\n",
    "    \n",
    "    Dense(128, activation='relu'),  # Reduced from 256 to 128 neurons\n",
    "    Dropout(0.3),                   # Reduced dropout rate for lighter regularization\n",
    "    \n",
    "    Dense(64, activation='relu'),   # Retained this layer but removed one hidden layer\n",
    "    \n",
    "    Dense(1, activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "ann_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717b1af-b2d0-4743-b13a-41cd12d9f41a",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd90b05-691e-4dcb-860c-d811fd001f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 1.2925 - accuracy: 0.7716 - val_loss: 0.1576 - val_accuracy: 0.9356\n",
      "Epoch 2/5\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1431 - accuracy: 0.9298 - val_loss: 0.0435 - val_accuracy: 0.9742\n",
      "Epoch 3/5\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9652 - val_loss: 0.0121 - val_accuracy: 0.9974\n",
      "Epoch 4/5\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0762 - accuracy: 0.9784 - val_loss: 0.0112 - val_accuracy: 0.9948\n",
      "Epoch 5/5\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0270 - accuracy: 0.9845 - val_loss: 0.0127 - val_accuracy: 0.9974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15774a210>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(300, 40)),  # Reduced filters from 64 to 32\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),  # Reduced dropout rate\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(32, activation='relu'),  # Reduced neurons from 64 to 32\n",
    "    Dropout(0.3),  # Reduced dropout rate\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d32838-ea1b-4121-b097-fa6c81279465",
   "metadata": {},
   "source": [
    "### CNN + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418e1e95-981a-486c-b95a-1b7f10f23884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       194\n",
      "           1       1.00      1.00      1.00       194\n",
      "\n",
      "    accuracy                           1.00       388\n",
      "   macro avg       1.00      1.00      1.00       388\n",
      "weighted avg       1.00      1.00      1.00       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "cnn_feature_extractor = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(300, 40)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten()\n",
    "])\n",
    "\n",
    "# Extract CNN features\n",
    "X_train_features = cnn_feature_extractor.predict(X_train)\n",
    "X_val_features = cnn_feature_extractor.predict(X_val)\n",
    "X_test_features = cnn_feature_extractor.predict(X_test)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
    "xgb_model.fit(X_train_features, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b82caa-95d3-48db-a80c-1e7e042b6f3d",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b764eb86-c38a-4a9d-84e7-5f694311a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - 24s 319ms/step - loss: 0.7335 - accuracy: 0.5221 - val_loss: 0.6097 - val_accuracy: 0.6392\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 17s 305ms/step - loss: 0.6414 - accuracy: 0.5686 - val_loss: 0.5843 - val_accuracy: 0.6624\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 18s 308ms/step - loss: 0.6245 - accuracy: 0.5835 - val_loss: 0.5711 - val_accuracy: 0.6624\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 17s 306ms/step - loss: 0.6114 - accuracy: 0.5852 - val_loss: 0.5663 - val_accuracy: 0.6624\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 18s 317ms/step - loss: 0.6018 - accuracy: 0.5946 - val_loss: 0.5672 - val_accuracy: 0.6649\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 18s 310ms/step - loss: 0.5957 - accuracy: 0.6018 - val_loss: 0.5677 - val_accuracy: 0.5438\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 18s 311ms/step - loss: 0.5987 - accuracy: 0.5946 - val_loss: 0.5538 - val_accuracy: 0.6701\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 18s 315ms/step - loss: 0.5871 - accuracy: 0.5985 - val_loss: 0.5493 - val_accuracy: 0.6727\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 17s 303ms/step - loss: 0.5827 - accuracy: 0.6012 - val_loss: 0.5564 - val_accuracy: 0.6727\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 17s 305ms/step - loss: 0.5863 - accuracy: 0.6106 - val_loss: 0.5514 - val_accuracy: 0.6727\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 18s 312ms/step - loss: 0.5725 - accuracy: 0.6206 - val_loss: 0.5399 - val_accuracy: 0.6753\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 18s 317ms/step - loss: 0.5866 - accuracy: 0.5962 - val_loss: 0.5428 - val_accuracy: 0.6727\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 18s 320ms/step - loss: 0.5698 - accuracy: 0.6139 - val_loss: 0.5317 - val_accuracy: 0.5747\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 18s 313ms/step - loss: 0.6395 - accuracy: 0.5824 - val_loss: 0.5727 - val_accuracy: 0.6495\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 19s 325ms/step - loss: 0.6040 - accuracy: 0.6034 - val_loss: 0.5572 - val_accuracy: 0.6649\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - 18s 314ms/step - loss: 0.5938 - accuracy: 0.5940 - val_loss: 0.5482 - val_accuracy: 0.6701\n",
      "Epoch 17/20\n",
      "57/57 [==============================] - 18s 319ms/step - loss: 0.5847 - accuracy: 0.5830 - val_loss: 0.5449 - val_accuracy: 0.6675\n",
      "Epoch 18/20\n",
      "57/57 [==============================] - 18s 323ms/step - loss: 0.5755 - accuracy: 0.6034 - val_loss: 0.5429 - val_accuracy: 0.6675\n",
      "Epoch 19/20\n",
      "57/57 [==============================] - 18s 319ms/step - loss: 0.5784 - accuracy: 0.6150 - val_loss: 0.5379 - val_accuracy: 0.6753\n",
      "Epoch 20/20\n",
      "57/57 [==============================] - 18s 318ms/step - loss: 0.5775 - accuracy: 0.6145 - val_loss: 0.5395 - val_accuracy: 0.6778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x157de0c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Improved RNN Model using LSTM\n",
    "rnn_model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(300, 40)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with a lower learning rate for better convergence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "rnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model with increased epochs for better learning\n",
    "rnn_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7b1b8-df8c-44c9-8df2-5c7a7a1121df",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "435399a8-c139-48db-bdb1-4ee4088054aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - 10s 150ms/step - loss: 0.6220 - accuracy: 0.5979 - val_loss: 0.5483 - val_accuracy: 0.6624\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 8s 143ms/step - loss: 0.5819 - accuracy: 0.6200 - val_loss: 0.5382 - val_accuracy: 0.6856\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 8s 145ms/step - loss: 0.5611 - accuracy: 0.6499 - val_loss: 0.5321 - val_accuracy: 0.6856\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.5350 - accuracy: 0.6759 - val_loss: 0.4724 - val_accuracy: 0.7474\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.4988 - accuracy: 0.7152 - val_loss: 0.3959 - val_accuracy: 0.8144\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 8s 147ms/step - loss: 0.3887 - accuracy: 0.8153 - val_loss: 0.2761 - val_accuracy: 0.8840\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.2276 - accuracy: 0.9032 - val_loss: 0.0807 - val_accuracy: 0.9716\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 8s 145ms/step - loss: 0.1069 - accuracy: 0.9668 - val_loss: 0.0579 - val_accuracy: 0.9820\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.1167 - accuracy: 0.9668 - val_loss: 0.2623 - val_accuracy: 0.9227\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.1441 - accuracy: 0.9508 - val_loss: 0.1076 - val_accuracy: 0.9768\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.0913 - accuracy: 0.9784 - val_loss: 0.0482 - val_accuracy: 0.9923\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 8s 145ms/step - loss: 0.0828 - accuracy: 0.9718 - val_loss: 0.0520 - val_accuracy: 0.9897\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.1265 - accuracy: 0.9685 - val_loss: 0.1207 - val_accuracy: 0.9691\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 8s 145ms/step - loss: 0.1935 - accuracy: 0.9430 - val_loss: 0.2350 - val_accuracy: 0.9510\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.1536 - accuracy: 0.9652 - val_loss: 0.0775 - val_accuracy: 0.9845\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.2438 - accuracy: 0.9132 - val_loss: 0.1419 - val_accuracy: 0.9665\n",
      "Epoch 17/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.1685 - accuracy: 0.9541 - val_loss: 0.0949 - val_accuracy: 0.9820\n",
      "Epoch 18/20\n",
      "57/57 [==============================] - 8s 146ms/step - loss: 0.1422 - accuracy: 0.9657 - val_loss: 0.0528 - val_accuracy: 0.9923\n",
      "Epoch 19/20\n",
      "57/57 [==============================] - 8s 145ms/step - loss: 0.0971 - accuracy: 0.9773 - val_loss: 0.0385 - val_accuracy: 0.9948\n",
      "Epoch 20/20\n",
      "57/57 [==============================] - 8s 144ms/step - loss: 0.0407 - accuracy: 0.9934 - val_loss: 0.0361 - val_accuracy: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x34caf9590>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(300, 40)),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4217822c-9611-4bf6-89e4-d97d35c3a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a results directory if it doesn't exist\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c7b9fcd-3506-4a00-8d02-4fd93ce11d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model_name, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Pause\", \"Pause\"], yticklabels=[\"Non-Pause\", \"Pause\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.savefig(f\"{results_dir}/{model_name}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc19de-d4a5-4df9-a660-20973e3e9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Select a model for SHAP and LIME explanations (change based on preference)\n",
    "selected_model = knn  # Example: Using k-NN\n",
    "selected_model_name = \"knn\"\n",
    "\n",
    "# Choose appropriate input data format for the selected model\n",
    "if selected_model_name in [\"knn\", \"rfe\", \"svm\", \"gmm\"]:\n",
    "    X_test_input = X_test.reshape(X_test.shape[0], -1)  # Flatten data for these models\n",
    "elif selected_model_name in [\"mlp_model\", \"ann_model\", \"cnn_model\", \"rnn_model\", \"lstm_model\"]:\n",
    "    X_test_input = X_test  # Keep original shape for neural networks\n",
    "elif selected_model_name == \"xgb_model\":\n",
    "    X_test_input = cnn_feature_extractor.predict(X_test)  # Use CNN extracted features for XGBoost\n",
    "\n",
    "# Generate SHAP values with a safe approach\n",
    "try:\n",
    "    explainer = shap.Explainer(selected_model.predict, X_test_input)\n",
    "    num_features = X_test_input.shape[1]\n",
    "    max_evals = max(1000, 2 * num_features + 1)  # Ensure max_evals is large enough\n",
    "    shap_values = explainer(X_test_input[:50], max_evals=max_evals)  # Adjust max_evals dynamically\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_test_input[:50], show=False)\n",
    "    plt.savefig(f\"results/shap_summary_{selected_model_name}.png\")\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(f\"SHAP computation failed: {e}\")\n",
    "\n",
    "# Generate LIME explanations with error handling\n",
    "try:\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X_test_input, mode='classification', feature_names=[f'feature_{i}' for i in range(X_test_input.shape[1])])\n",
    "    idx = np.random.randint(0, len(X_test_input))\n",
    "    exp = explainer.explain_instance(X_test_input[idx], selected_model.predict_proba)\n",
    "    \n",
    "    # Save LIME explanation as PNG\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    fig.savefig(f\"results/lime_explanation_{selected_model_name}.png\")\n",
    "    plt.close(fig)\n",
    "except Exception as e:\n",
    "    print(f\"LIME computation failed: {e}\")\n",
    "\n",
    "# Generate t-SNE visualization safely\n",
    "try:\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=500)\n",
    "    X_test_tsne = tsne.fit_transform(X_test_input)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_test_tsne[:, 0], X_test_tsne[:, 1], c=y_test, cmap='coolwarm', alpha=0.7)\n",
    "    plt.colorbar(label=\"Class Label\")\n",
    "    plt.title(\"t-SNE Visualization of Test Data\")\n",
    "    plt.savefig(\"results/tsne_visualization.png\")\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(f\"t-SNE computation failed: {e}\")\n",
    "\n",
    "print(\"SHAP, LIME, and t-SNE visualizations saved in 'results' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcb47ed2-dc53-4deb-a86c-691c1f1991d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 1s 83ms/step\n",
      "13/13 [==============================] - 1s 35ms/step\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m df_metrics\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_performance_metrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Display metrics table\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m     75\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Performance Metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mdf_metrics)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Compute and return evaluation metrics for a model.\"\"\"\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_prob) if y_prob is not None else \"N/A\"\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Group Models by Type\n",
    "statistical_models = {\n",
    "    \"Gaussian Mixture Models\": gmm\n",
    "}\n",
    "\n",
    "machine_learning_models = {\n",
    "    \"Support Vector Machine\": svm,\n",
    "    \"Random Forest (RFE)\": rfe,\n",
    "    \"K-Nearest Neighbour\": knn,\n",
    "}\n",
    "\n",
    "deep_learning_models = {\n",
    "    \"Multilayer Perceptron\": mlp_model,\n",
    "    \"Artificial Neural Network\": ann_model,\n",
    "    \"Convolutional Neural Network\": cnn_model,\n",
    "    \"CNN + XGBoost\": xgb_model,\n",
    "    \"Recurrent Neural Network\": rnn_model,\n",
    "    \"Long Short-Term Memory\": lstm_model\n",
    "}\n",
    "\n",
    "# Store results separately for each model type\n",
    "metrics_table = []\n",
    "\n",
    "# Evaluate Statistical Models\n",
    "for model_name, model in statistical_models.items():\n",
    "    y_pred = model.predict(X_test_flat)\n",
    "    y_prob = None  # Most statistical models don't provide probability outputs\n",
    "    metrics_table.append(evaluate_model(f\"Statistical | {model_name}\", y_test, y_pred, y_prob))\n",
    "\n",
    "# Evaluate Machine Learning Models\n",
    "for model_name, model in machine_learning_models.items():\n",
    "    y_pred = model.predict(X_test_flat)\n",
    "    y_prob = model.predict_proba(X_test_flat)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    metrics_table.append(evaluate_model(f\"ML | {model_name}\", y_test, y_pred, y_prob))\n",
    "\n",
    "# Evaluate Deep Learning Models\n",
    "# Deep Learning Models - Ensure correct input for CNN + XGBoost\n",
    "for model_name, model in deep_learning_models.items():\n",
    "    if model_name == \"CNN + XGBoost\":\n",
    "        # Extract CNN Features before using XGBoost\n",
    "        X_test_features = cnn_feature_extractor.predict(X_test)  # Extract CNN features\n",
    "        y_pred = model.predict(X_test_features)  # Predict using XGBoost\n",
    "        y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary\n",
    "        y_prob = y_pred.flatten()\n",
    "    else:\n",
    "        # Other DL models work with 3D input\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "        y_prob = y_pred.flatten()\n",
    "\n",
    "    metrics_table.append(evaluate_model(f\"DL | {model_name}\", y_test, y_prob, y_prob))\n",
    "\n",
    "\n",
    "# Convert to DataFrame and Save\n",
    "df_metrics = pd.DataFrame(metrics_table)\n",
    "df_metrics.to_csv(f\"{results_dir}/model_performance_metrics.csv\", index=False)\n",
    "\n",
    "# Display metrics table\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Model Performance Metrics\", dataframe=df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce22aa3-d155-468d-bcc9-23a2c4d0eb56",
   "metadata": {},
   "source": [
    "## For KNN, RFE, SVM, GMM (Flattened MFCC Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92f08a-8ef0-41fe-8166-6c6b123c0544",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning Models (SVM, Decision Trees, Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096e7f61-bcae-4ddf-9a9a-020e6b7d6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def trad_shap(model, X_test):\n",
    "    # Assuming you have a trained model and test data\n",
    "    explainer = shap.Explainer(model, X_test)  \n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    # Visualize global feature importance\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=[f'MFCC_{i}' for i in range(X_test.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc644c94-c1b0-48ee-92b9-6408b0e4b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trad_permu_imp(model, X_test, y_test):\n",
    "    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    importances = result.importances_mean\n",
    "    \n",
    "    plt.barh(range(len(importances)), importances)\n",
    "    plt.yticks(range(len(importances)), [f'MFCC_{i}' for i in range(X_test.shape[1])])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.ylabel(\"MFCC Features\")\n",
    "    plt.title(\"Permutation Feature Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedf49c-41f6-4a53-bd6d-9378e3f71878",
   "metadata": {},
   "source": [
    "## Deep Learning Models (CNN, ANN, RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8ecda7-ab37-4542-96bb-29bd9b6ac2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def grad_cam(model, X_test):\n",
    "    # Assuming TensorFlow model\n",
    "    explainer = shap.Explainer(model, X_test)\n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    # Convert attributions to NumPy for visualization\n",
    "    attr = np.mean(shap_values.values, axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.barh(range(len(attr)), attr)\n",
    "    plt.yticks(range(len(attr)), [f'MFCC_{i}' for i in range(len(attr))])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.ylabel(\"MFCC Features\")\n",
    "    plt.title(\"SHAP Feature Importance for MFCC Features\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb49be-2fc3-4516-9587-d3a679ebf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load JSON and audio data\n",
    "json_folder = 'JSON'\n",
    "audio_folder = 'cleaned_wav_files'\n",
    "\n",
    "def load_annotations(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    annotations = [(item['value']['start'], item['value']['end'], item['value']['labels'][0])\n",
    "                   for item in data[0]['annotations'][0]['result'] if item['type'] == 'labels']\n",
    "    return annotations\n",
    "\n",
    "# Feature Extraction (MFCC)\n",
    "def extract_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfcc, axis=1)\n",
    "\n",
    "# Data Preprocessing\n",
    "audio_files = {os.path.splitext(f)[0]: os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(('.wav', '.m4a', '.mp3'))}\n",
    "data, labels = [], []\n",
    "for name, path in audio_files.items():\n",
    "    features = extract_features(path)\n",
    "    label = name.split('_')[0]  # Assuming label is in filename\n",
    "    data.append(features)\n",
    "    labels.append(label)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Selection\n",
    "models = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"svm\": SVC(probability=True),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=100),\n",
    "    \"xgb\": XGBClassifier(),\n",
    "    \"cnn\": Sequential([Conv1D(32, 3, activation='relu', input_shape=(13, 1)), Flatten(), Dense(10, activation='softmax')]),\n",
    "    \"lstm\": Sequential([LSTM(50, return_sequences=True, input_shape=(13, 1)), Flatten(), Dense(10, activation='softmax')])\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if name in [\"cnn\", \"lstm\"]:\n",
    "        X_train_nn = X_train.reshape(-1, 13, 1)\n",
    "        X_test_nn = X_test.reshape(-1, 13, 1)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train_nn, y_train, epochs=10, batch_size=16, verbose=0)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "# Explainability with SHAP\n",
    "explainer = shap.Explainer(models[\"rf\"], X_train)\n",
    "shap_values = explainer(X_test[:10])\n",
    "shap.summary_plot(shap_values, X_test[:10])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_filled_pause)",
   "language": "python",
   "name": "venv_filled_pause"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
