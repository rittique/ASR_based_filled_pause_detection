{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6a5449-bb9f-4dff-8d08-b27f034f2b58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m     save_generated_audio(generator, output_dir)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     main(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cleaned_wav_files\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    163\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./JSON\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    164\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dummy_audios\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 156\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(audio_dir, annotation_dir, output_dir)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(audio_dir, annotation_dir, output_dir):\n\u001b[1;32m    155\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m AudioDataset(audio_dir, annotation_dir)\n\u001b[0;32m--> 156\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprepare_dataset()\n\u001b[1;32m    158\u001b[0m     generator \u001b[38;5;241m=\u001b[39m train_gan(processed_data)\n\u001b[1;32m    159\u001b[0m     save_generated_audio(generator, output_dir)\n",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m, in \u001b[0;36mAudioDataset.prepare_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m     61\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 62\u001b[0m normalized_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(np\u001b[38;5;241m.\u001b[39marray(audio_data)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(normalized_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    489\u001b[0m     X,\n\u001b[1;32m    490\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[1;32m    491\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    492\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    493\u001b[0m )\n\u001b[1;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class AudioDataset:\n",
    "    def __init__(self, audio_dir, annotation_dir):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.audio_files = self._get_matched_files()\n",
    "    \n",
    "    def _get_matched_files(self):\n",
    "        \"\"\"\n",
    "        Find audio files with corresponding annotation files\n",
    "        \"\"\"\n",
    "        matched_files = []\n",
    "        for audio_file in os.listdir(self.audio_dir):\n",
    "            base_name = os.path.splitext(audio_file)[0]\n",
    "            annotation_path = os.path.join(self.annotation_dir, base_name + '.txt')\n",
    "            \n",
    "            if os.path.exists(annotation_path):\n",
    "                matched_files.append(os.path.join(self.audio_dir, audio_file))\n",
    "        \n",
    "        return matched_files\n",
    "    \n",
    "    def load_audio(self, file_path, target_sr=16000, max_duration=3):\n",
    "        \"\"\"\n",
    "        Load and preprocess audio file\n",
    "        \"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sample_rate != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Trim or pad to fixed length\n",
    "        max_length = max_duration * target_sr\n",
    "        if waveform.shape[1] > max_length:\n",
    "            waveform = waveform[:, :max_length]\n",
    "        elif waveform.shape[1] < max_length:\n",
    "            pad_length = max_length - waveform.shape[1]\n",
    "            waveform = nn.functional.pad(waveform, (0, pad_length))\n",
    "        \n",
    "        return waveform.squeeze()\n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare normalized audio data\n",
    "        \"\"\"\n",
    "        audio_data = []\n",
    "        for file in self.audio_files:\n",
    "            audio = self.load_audio(file)\n",
    "            audio_data.append(audio.numpy())\n",
    "        \n",
    "        # Normalize\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        normalized_data = scaler.fit_transform(np.array(audio_data).reshape(-1, 1)).reshape(-1)\n",
    "        \n",
    "        return torch.tensor(normalized_data, dtype=torch.float32)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_gan(dataset, latent_dim=100, epochs=200, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train GAN for synthetic audio generation\n",
    "    \"\"\"\n",
    "    generator = Generator(latent_dim, dataset.shape[1])\n",
    "    discriminator = Discriminator(dataset.shape[1])\n",
    "    \n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        real_data = dataset\n",
    "        z = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(z)\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        real_loss = criterion(discriminator(real_data), real_labels)\n",
    "        fake_loss = criterion(discriminator(fake_data.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        \n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train generator\n",
    "        g_optimizer.zero_grad()\n",
    "        z = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(z)\n",
    "        g_loss = criterion(discriminator(fake_data), real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    return generator\n",
    "\n",
    "def save_generated_audio(generator, output_dir, num_samples=10, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Generate and save synthetic audio samples\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, 100)\n",
    "        generated_audio = generator(z).numpy()\n",
    "        \n",
    "        for i, audio in enumerate(generated_audio):\n",
    "            wavfile.write(\n",
    "                os.path.join(output_dir, f'synthetic_audio_{i}.wav'), \n",
    "                sample_rate, \n",
    "                audio\n",
    "            )\n",
    "\n",
    "def main(audio_dir, annotation_dir, output_dir):\n",
    "    dataset = AudioDataset(audio_dir, annotation_dir)\n",
    "    processed_data = dataset.prepare_dataset()\n",
    "    \n",
    "    generator = train_gan(processed_data)\n",
    "    save_generated_audio(generator, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('./cleaned_wav_files', \n",
    "         './JSON', \n",
    "         './dummy_audios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a22199-1823-4fc9-84c4-d348afecd04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Voice 007_sd.wav',\n",
       " 'R1.wav',\n",
       " 'Voice 004.wav',\n",
       " 'omg dj.wav',\n",
       " 'practice .wav',\n",
       " 'Voice 006.wav',\n",
       " 'Voice 001_sd (1).wav',\n",
       " 'Sep 25, 1.56 sa.wav',\n",
       " 'R3.wav',\n",
       " 'R2.wav',\n",
       " 'Voice 007.wav',\n",
       " 'hjjjhhggh.wav',\n",
       " 'tamanna.wav',\n",
       " 'Voice 003.wav',\n",
       " 'Oct 8, 1.45 PM.wav',\n",
       " 'R7.wav',\n",
       " 'Voice 002.wav',\n",
       " 'cricket toppic.wav',\n",
       " 'Sep 26, 1.30 PM.wav',\n",
       " 'Voice 009_sd.wav',\n",
       " 'R5.wav',\n",
       " 'R4.wav',\n",
       " 'Voice 001.wav',\n",
       " 'Voice 001 (2).wav',\n",
       " 'Account.wav',\n",
       " 'kerala.wav',\n",
       " 'Voice 001_sd.wav',\n",
       " 'R10.wav',\n",
       " 'REC005.wav',\n",
       " 'Voice 002 (2).wav',\n",
       " 'progga.wav',\n",
       " 'Oct 20, 12.28 PM.wav',\n",
       " 'English 1st conversation.wav',\n",
       " 'SS practice.wav',\n",
       " '2024-02-27 21-05-26.wav',\n",
       " 'Voice 002 (1).wav',\n",
       " 'keralas calture.wav',\n",
       " 'New Recording 6.wav',\n",
       " 'Recode.wav',\n",
       " 'Voice 008_sd.wav',\n",
       " 'Mirpur Road.wav',\n",
       " 'summery.wav',\n",
       " 'Oct hhjh.wav',\n",
       " 'R9.wav',\n",
       " 'the scientist.wav',\n",
       " 'R8.wav',\n",
       " 'Voice 001 (1).wav',\n",
       " 'sanjid meyad.wav',\n",
       " 'Oct 7, 9.34 AM.wav',\n",
       " 'Recording_2.wav',\n",
       " 'Sep 23, 9.24 AM.wav',\n",
       " 'Voice 010_sd.wav',\n",
       " 'Recording_1.wav',\n",
       " 'Oct 6, 9.10 AM.wav']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./cleaned_wav_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235fa89d-93c2-424f-ac1b-1feb1afc5371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D Loss = 1.390404462814331, G Loss = 1.435276746749878\n",
      "Epoch 10: D Loss = 1.5485504865646362, G Loss = 1.465993046760559\n",
      "Epoch 20: D Loss = 0.6792703866958618, G Loss = 2.950965642929077\n",
      "Epoch 30: D Loss = 0.19791191816329956, G Loss = 4.28369140625\n",
      "Epoch 40: D Loss = 1.5522111654281616, G Loss = 1.8755478858947754\n",
      "Epoch 50: D Loss = 1.364415168762207, G Loss = 0.8510583639144897\n",
      "Epoch 60: D Loss = 0.8493151068687439, G Loss = 1.168866515159607\n",
      "Epoch 70: D Loss = 0.6472721695899963, G Loss = 2.6100823879241943\n",
      "Epoch 80: D Loss = 0.27620455622673035, G Loss = 2.524670362472534\n",
      "Epoch 90: D Loss = 0.2219024896621704, G Loss = 2.7784674167633057\n",
      "Epoch 100: D Loss = 0.3148179054260254, G Loss = 2.6815073490142822\n",
      "Epoch 110: D Loss = 0.3387452960014343, G Loss = 2.319704294204712\n",
      "Epoch 120: D Loss = 0.7214686274528503, G Loss = 3.800121784210205\n",
      "Epoch 130: D Loss = 0.44992297887802124, G Loss = 1.609717845916748\n",
      "Epoch 140: D Loss = 0.20898646116256714, G Loss = 3.567080020904541\n",
      "Epoch 150: D Loss = 0.18065571784973145, G Loss = 3.4543464183807373\n",
      "Epoch 160: D Loss = 0.14727219939231873, G Loss = 3.2850310802459717\n",
      "Epoch 170: D Loss = 0.15306033194065094, G Loss = 3.75219464302063\n",
      "Epoch 180: D Loss = 0.12046782672405243, G Loss = 3.953622341156006\n",
      "Epoch 190: D Loss = 0.09805454313755035, G Loss = 4.483214378356934\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AudioDataset:\n",
    "    def __init__(self, audio_dir, annotation_dir):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.audio_files = self._get_matched_files()\n",
    "        \n",
    "        if not self.audio_files:\n",
    "            raise ValueError(f\"No matched audio files found in {audio_dir}\")\n",
    "    \n",
    "    def _get_matched_files(self):\n",
    "        \"\"\"Find audio files with corresponding annotation files\"\"\"\n",
    "        matched_files = []\n",
    "        for audio_file in os.listdir(self.audio_dir):\n",
    "            base_name = os.path.splitext(audio_file)[0]\n",
    "            annotation_path = os.path.join(self.annotation_dir, base_name + '.json')\n",
    "            \n",
    "            if os.path.exists(annotation_path):\n",
    "                matched_files.append(os.path.join(self.audio_dir, audio_file))\n",
    "        \n",
    "        return matched_files\n",
    "    \n",
    "    def load_audio(self, file_path, target_sr=16000, max_duration=3):\n",
    "        \"\"\"Load and preprocess individual audio file\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            \n",
    "            # Convert to mono if stereo\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sample_rate != target_sr:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, target_sr)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # Trim or pad to fixed length\n",
    "            max_length = max_duration * target_sr\n",
    "            if waveform.shape[1] > max_length:\n",
    "                waveform = waveform[:, :max_length]\n",
    "            elif waveform.shape[1] < max_length:\n",
    "                pad_length = max_length - waveform.shape[1]\n",
    "                waveform = nn.functional.pad(waveform, (0, pad_length))\n",
    "            \n",
    "            return waveform.squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"Prepare normalized audio data\"\"\"\n",
    "        audio_data = []\n",
    "        for file in self.audio_files:\n",
    "            audio = self.load_audio(file)\n",
    "            if audio is not None:\n",
    "                audio_data.append(audio.numpy())\n",
    "        \n",
    "        if not audio_data:\n",
    "            raise ValueError(\"No valid audio files could be loaded\")\n",
    "        \n",
    "        # Reshape and normalize\n",
    "        audio_data = np.array(audio_data)\n",
    "        audio_data = audio_data.reshape(audio_data.shape[0], -1)\n",
    "        \n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        normalized_data = scaler.fit_transform(audio_data)\n",
    "        \n",
    "        return {\n",
    "            'data': torch.tensor(normalized_data, dtype=torch.float32),\n",
    "            'scaler': scaler\n",
    "        }\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_gan(dataset, latent_dim=100, epochs=200, batch_size=64):\n",
    "    \"\"\"Train GAN for synthetic audio generation\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test = train_test_split(dataset['data'], test_size=0.2)\n",
    "    \n",
    "    generator = Generator(latent_dim, X_train.shape[1])\n",
    "    discriminator = Discriminator(X_train.shape[1])\n",
    "    \n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch = X_train[i:i+batch_size]\n",
    "            \n",
    "            # Train discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            real_data = batch\n",
    "            z = torch.randn(len(batch), latent_dim)\n",
    "            fake_data = generator(z)\n",
    "            \n",
    "            real_labels = torch.ones(len(batch), 1)\n",
    "            fake_labels = torch.zeros(len(batch), 1)\n",
    "            \n",
    "            real_loss = criterion(discriminator(real_data), real_labels)\n",
    "            fake_loss = criterion(discriminator(fake_data.detach()), fake_labels)\n",
    "            d_loss = real_loss + fake_loss\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Train generator\n",
    "            g_optimizer.zero_grad()\n",
    "            z = torch.randn(len(batch), latent_dim)\n",
    "            fake_data = generator(z)\n",
    "            g_loss = criterion(discriminator(fake_data), torch.ones(len(batch), 1))\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "        # Optional: Print epoch loss\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: D Loss = {d_loss.item()}, G Loss = {g_loss.item()}\")\n",
    "    \n",
    "    return generator, dataset['scaler']\n",
    "\n",
    "def save_generated_audio(generator, scaler, output_dir, num_samples=10, sample_rate=16000):\n",
    "    \"\"\"Generate and save synthetic audio samples\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, 100)\n",
    "        generated_audio = generator(z).numpy()\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        generated_audio = scaler.inverse_transform(generated_audio)\n",
    "        \n",
    "        for i, audio in enumerate(generated_audio):\n",
    "            # Normalize to int16 range\n",
    "            audio = audio.reshape(-1)\n",
    "            audio = (audio / np.max(np.abs(audio)) * 32767).astype(np.int16)\n",
    "            \n",
    "            wavfile.write(\n",
    "                os.path.join(output_dir, f'synthetic_audio_{i}.wav'), \n",
    "                sample_rate, \n",
    "                audio\n",
    "            )\n",
    "\n",
    "def main(audio_dir, annotation_dir, output_dir):\n",
    "    # Initialize dataset\n",
    "    dataset = AudioDataset(audio_dir, annotation_dir)\n",
    "    \n",
    "    # Prepare data\n",
    "    processed_data = dataset.prepare_dataset()\n",
    "    \n",
    "    # Train GAN\n",
    "    generator, scaler = train_gan(processed_data)\n",
    "    \n",
    "    # Generate and save synthetic audio\n",
    "    save_generated_audio(generator, scaler, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('./cleaned_wav_files', \n",
    "         './JSON', \n",
    "         './dummy_audios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1165c5a4-331c-478d-84e0-5b4087ae6f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AudioDataset' object has no attribute '_get_matched_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 161\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    160\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     main(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cleaned_wav_files\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./JSON\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./generated_audio\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(audio_dir, annotation_dir, output_dir, device)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(audio_dir, annotation_dir, output_dir, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 154\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m AudioDataset(audio_dir, annotation_dir)\n\u001b[1;32m    155\u001b[0m     processed_data, scaler \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprepare_dataset()\n\u001b[1;32m    156\u001b[0m     generator, _ \u001b[38;5;241m=\u001b[39m train_gan(processed_data, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[0;34m(self, audio_dir, annotation_dir, target_sr, max_duration)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_duration \u001b[38;5;241m=\u001b[39m max_duration\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_processor \u001b[38;5;241m=\u001b[39m AudioProcessor(sr\u001b[38;5;241m=\u001b[39mtarget_sr)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_matched_files()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AudioDataset' object has no attribute '_get_matched_files'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import noisereduce as nr\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sr=16000):\n",
    "        self.sr = sr\n",
    "        self.pause_detector = self._initialize_pause_detector()\n",
    "    \n",
    "    def _initialize_pause_detector(self):\n",
    "        keywords = ['aaah', 'mmm', 'hmm', 'uhh', 'umm', 'err']\n",
    "        return {word: librosa.sequence.dtw for word in keywords}\n",
    "    \n",
    "    def reduce_noise(self, audio):\n",
    "        reduced_noise = nr.reduce_noise(\n",
    "            y=audio,\n",
    "            sr=self.sr,\n",
    "            prop_decrease=0.95,\n",
    "            n_fft=2048,\n",
    "            win_length=2048,\n",
    "            hop_length=512\n",
    "        )\n",
    "        return reduced_noise\n",
    "    \n",
    "    def detect_pauses(self, audio):\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=self.sr, n_mfcc=13)\n",
    "        segments = librosa.effects.split(audio, top_db=20)\n",
    "        \n",
    "        pauses = []\n",
    "        for start, end in segments:\n",
    "            segment = audio[start:end]\n",
    "            segment_mfcc = librosa.feature.mfcc(y=segment, sr=self.sr, n_mfcc=13)\n",
    "            \n",
    "            is_filled_pause = False\n",
    "            for keyword, dtw in self.pause_detector.items():\n",
    "                if len(segment) > self.sr * 0.1:  # Longer than 100ms\n",
    "                    is_filled_pause = True\n",
    "                    break\n",
    "            \n",
    "            pauses.append({\n",
    "                'start': float(start / self.sr),\n",
    "                'end': float(end / self.sr),\n",
    "                'type': 'filled_pause' if is_filled_pause else 'non_filled_pause'\n",
    "            })\n",
    "        \n",
    "        return pauses\n",
    "\n",
    "class AudioDataset:\n",
    "    def __init__(self, audio_dir, annotation_dir, target_sr=16000, max_duration=3):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.max_duration = max_duration\n",
    "        self.audio_processor = AudioProcessor(sr=target_sr)\n",
    "        self.audio_files = self._get_matched_files()\n",
    "    \n",
    "    def load_and_process_audio(self, file_path):\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            \n",
    "            # Convert to mono\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sr != self.target_sr:\n",
    "                waveform = torchaudio.transforms.Resample(sr, self.target_sr)(waveform)\n",
    "            \n",
    "            # Convert to numpy for noise reduction\n",
    "            audio_np = waveform.squeeze().numpy()\n",
    "            cleaned_audio = self.audio_processor.reduce_noise(audio_np)\n",
    "            \n",
    "            # Process fixed length\n",
    "            max_length = self.max_duration * self.target_sr\n",
    "            if len(cleaned_audio) > max_length:\n",
    "                cleaned_audio = cleaned_audio[:max_length]\n",
    "            else:\n",
    "                pad_length = max_length - len(cleaned_audio)\n",
    "                cleaned_audio = np.pad(cleaned_audio, (0, pad_length))\n",
    "            \n",
    "            return cleaned_audio\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_dataset(self, batch_size=32):\n",
    "        processed_data = []\n",
    "        annotations = []\n",
    "        \n",
    "        for file in self.audio_files:\n",
    "            if audio := self.load_and_process_audio(file):\n",
    "                processed_data.append(audio)\n",
    "                pauses = self.audio_processor.detect_pauses(audio)\n",
    "                annotations.append(pauses)\n",
    "        \n",
    "        if not processed_data:\n",
    "            raise ValueError(\"No valid audio files could be processed\")\n",
    "        \n",
    "        processed_data = np.array(processed_data)\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        normalized_data = scaler.fit_transform(processed_data.reshape(len(processed_data), -1))\n",
    "        \n",
    "        return self._create_dataloaders(normalized_data, annotations, batch_size), scaler\n",
    "\n",
    "def save_generated_audio_with_annotations(generator, scaler, output_dir, num_samples=10, \n",
    "                                        sample_rate=16000, device='cuda'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'annotations'), exist_ok=True)\n",
    "    \n",
    "    audio_processor = AudioProcessor(sr=sample_rate)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, generator.model[0].in_features, device=device)\n",
    "        generated_audio = generator(z).cpu().numpy()\n",
    "        generated_audio = scaler.inverse_transform(generated_audio)\n",
    "        \n",
    "        for i, audio in enumerate(generated_audio):\n",
    "            # Save audio\n",
    "            audio = audio.reshape(-1)\n",
    "            audio = np.clip(audio, -1, 1)\n",
    "            audio_int16 = (audio * 32767).astype(np.int16)\n",
    "            \n",
    "            audio_path = os.path.join(output_dir, f'synthetic_audio_{i}.wav')\n",
    "            wavfile.write(audio_path, sample_rate, audio_int16)\n",
    "            \n",
    "            # Generate and save annotations\n",
    "            pauses = audio_processor.detect_pauses(audio)\n",
    "            annotation = {\n",
    "                'audio_file': f'synthetic_audio_{i}.wav',\n",
    "                'sample_rate': sample_rate,\n",
    "                'duration': len(audio) / sample_rate,\n",
    "                'pauses': pauses\n",
    "            }\n",
    "            \n",
    "            json_path = os.path.join(output_dir, 'annotations', f'synthetic_audio_{i}.json')\n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(annotation, f, indent=2)\n",
    "\n",
    "# [Previous WGAN and other model classes remain the same]\n",
    "\n",
    "def main(audio_dir, annotation_dir, output_dir, device='cuda'):\n",
    "    dataset = AudioDataset(audio_dir, annotation_dir)\n",
    "    processed_data, scaler = dataset.prepare_dataset()\n",
    "    generator, _ = train_gan(processed_data, device=device)\n",
    "    save_generated_audio_with_annotations(generator, scaler, output_dir, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    main('./cleaned_wav_files', './JSON', './generated_audio', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22809ce4-ebc0-4cc3-bfc8-edde71b1227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D Loss = -0.5372, G Loss = -0.3834\n",
      "Epoch 10: D Loss = -0.9829, G Loss = -0.0128\n",
      "Epoch 20: D Loss = -0.9924, G Loss = -0.0047\n",
      "Epoch 30: D Loss = -0.9956, G Loss = -0.0026\n",
      "Epoch 40: D Loss = -0.9972, G Loss = -0.0020\n",
      "Epoch 50: D Loss = -0.9988, G Loss = -0.0013\n",
      "Epoch 60: D Loss = -0.9989, G Loss = -0.0009\n",
      "Epoch 70: D Loss = -0.9993, G Loss = -0.0005\n",
      "Epoch 80: D Loss = -0.9996, G Loss = -0.0005\n",
      "Epoch 90: D Loss = -0.9998, G Loss = -0.0002\n",
      "Epoch 100: D Loss = -0.9998, G Loss = -0.0003\n",
      "Epoch 110: D Loss = -0.9999, G Loss = -0.0001\n",
      "Epoch 120: D Loss = -0.9999, G Loss = -0.0001\n",
      "Epoch 130: D Loss = -0.9998, G Loss = -0.0001\n",
      "Epoch 140: D Loss = -0.9999, G Loss = -0.0001\n",
      "Epoch 150: D Loss = -1.0000, G Loss = -0.0000\n",
      "Epoch 160: D Loss = -1.0000, G Loss = -0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import noisereduce as nr\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sr=16000):\n",
    "        self.sr = sr\n",
    "        self.pause_detector = self._initialize_pause_detector()\n",
    "    \n",
    "    def _initialize_pause_detector(self):\n",
    "        keywords = ['aaah', 'mmm', 'hmm', 'uhh', 'umm', 'err']\n",
    "        return {word: librosa.sequence.dtw for word in keywords}\n",
    "    \n",
    "    def reduce_noise(self, audio):\n",
    "        return nr.reduce_noise(\n",
    "            y=audio,\n",
    "            sr=self.sr,\n",
    "            prop_decrease=0.95,\n",
    "            n_fft=2048,\n",
    "            win_length=2048,\n",
    "            hop_length=512\n",
    "        )\n",
    "    \n",
    "    def detect_pauses(self, audio):\n",
    "        segments = librosa.effects.split(audio, top_db=20)\n",
    "        pauses = []\n",
    "        \n",
    "        for start, end in segments:\n",
    "            segment = audio[start:end]\n",
    "            is_filled_pause = len(segment) > self.sr * 0.1\n",
    "            \n",
    "            pauses.append({\n",
    "                'start': float(start / self.sr),\n",
    "                'end': float(end / self.sr),\n",
    "                'type': 'filled_pause' if is_filled_pause else 'non_filled_pause'\n",
    "            })\n",
    "        \n",
    "        return pauses\n",
    "\n",
    "class AudioDataset:\n",
    "    def __init__(self, audio_dir, annotation_dir, target_sr=16000, max_duration=3):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.max_duration = max_duration\n",
    "        self.audio_processor = AudioProcessor(sr=target_sr)\n",
    "        self.audio_files = self._get_matched_files()\n",
    "        \n",
    "        if not self.audio_files:\n",
    "            raise ValueError(f\"No matched audio files found in {audio_dir}\")\n",
    "    \n",
    "    def _get_matched_files(self):\n",
    "        audio_files = []\n",
    "        for audio_file in os.listdir(self.audio_dir):\n",
    "            base_name = os.path.splitext(audio_file)[0]\n",
    "            annotation_path = os.path.join(self.annotation_dir, f\"{base_name}.json\")\n",
    "            \n",
    "            if os.path.exists(annotation_path):\n",
    "                audio_files.append(os.path.join(self.audio_dir, audio_file))\n",
    "        return audio_files\n",
    "    \n",
    "    def load_and_process_audio(self, file_path):\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            \n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "            if sr != self.target_sr:\n",
    "                waveform = torchaudio.transforms.Resample(sr, self.target_sr)(waveform)\n",
    "            \n",
    "            audio_np = waveform.squeeze().numpy()\n",
    "            \n",
    "            if len(audio_np) > 0:\n",
    "                cleaned_audio = self.audio_processor.reduce_noise(audio_np)\n",
    "                \n",
    "                max_length = self.max_duration * self.target_sr\n",
    "                if len(cleaned_audio) > max_length:\n",
    "                    cleaned_audio = cleaned_audio[:max_length]\n",
    "                else:\n",
    "                    pad_length = max_length - len(cleaned_audio)\n",
    "                    cleaned_audio = np.pad(cleaned_audio, (0, pad_length))\n",
    "                \n",
    "                return cleaned_audio\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def prepare_dataset(self, batch_size=32):\n",
    "        processed_data = []\n",
    "        annotations = []\n",
    "        \n",
    "        for file in self.audio_files:\n",
    "            audio = self.load_and_process_audio(file)\n",
    "            if audio is not None:\n",
    "                processed_data.append(audio)\n",
    "                pauses = self.audio_processor.detect_pauses(audio)\n",
    "                annotations.append(pauses)\n",
    "        \n",
    "        if not processed_data:\n",
    "            raise ValueError(\"No valid audio files could be processed\")\n",
    "        \n",
    "        processed_data = np.array(processed_data)\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        normalized_data = scaler.fit_transform(processed_data.reshape(len(processed_data), -1))\n",
    "        tensor_data = torch.tensor(normalized_data, dtype=torch.float32)\n",
    "        \n",
    "        train_data, test_data = train_test_split(tensor_data, test_size=0.2)\n",
    "        train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(TensorDataset(test_data), batch_size=batch_size)\n",
    "        \n",
    "        return {'train_loader': train_loader, 'test_loader': test_loader, 'scaler': scaler}\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class WGAN(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = Generator(latent_dim, output_dim).to(device)\n",
    "        self.discriminator = Discriminator(output_dim).to(device)\n",
    "        \n",
    "        self.g_optimizer = optim.RMSprop(self.generator.parameters(), lr=0.00005)\n",
    "        self.d_optimizer = optim.RMSprop(self.discriminator.parameters(), lr=0.00005)\n",
    "    \n",
    "    def train_step(self, real_data):\n",
    "        batch_size = real_data.size(0)\n",
    "        real_data = real_data.to(self.device)\n",
    "        \n",
    "        # Train discriminator\n",
    "        for _ in range(5):\n",
    "            self.d_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
    "            fake_data = self.generator(z)\n",
    "            \n",
    "            d_real = self.discriminator(real_data)\n",
    "            d_fake = self.discriminator(fake_data.detach())\n",
    "            \n",
    "            d_loss = -(torch.mean(d_real) - torch.mean(d_fake))\n",
    "            d_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            # Weight clipping\n",
    "            for p in self.discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # Train generator\n",
    "        self.g_optimizer.zero_grad()\n",
    "        fake_data = self.generator(z)\n",
    "        g_loss = -torch.mean(self.discriminator(fake_data))\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "        \n",
    "        return d_loss.item(), g_loss.item()\n",
    "\n",
    "def train_gan(dataset, latent_dim=100, epochs=200, device='cuda'):\n",
    "    output_dim = next(iter(dataset['train_loader']))[0].shape[1]\n",
    "    model = WGAN(latent_dim, output_dim, device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        d_losses, g_losses = [], []\n",
    "        \n",
    "        for batch in dataset['train_loader']:\n",
    "            d_loss, g_loss = model.train_step(batch[0])\n",
    "            d_losses.append(d_loss)\n",
    "            g_losses.append(g_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: D Loss = {np.mean(d_losses):.4f}, G Loss = {np.mean(g_losses):.4f}\")\n",
    "    \n",
    "    return model.generator, dataset['scaler']\n",
    "\n",
    "def save_generated_audio_with_annotations(generator, scaler, output_dir, num_samples=10, \n",
    "                                        sample_rate=16000, device='cuda'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'annotations'), exist_ok=True)\n",
    "    \n",
    "    audio_processor = AudioProcessor(sr=sample_rate)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, generator.model[0].in_features, device=device)\n",
    "        generated_audio = generator(z).cpu().numpy()\n",
    "        generated_audio = scaler.inverse_transform(generated_audio)\n",
    "        \n",
    "        for i, audio in enumerate(generated_audio):\n",
    "            # Save audio\n",
    "            audio = audio.reshape(-1)\n",
    "            audio = np.clip(audio, -1, 1)\n",
    "            audio_int16 = (audio * 32767).astype(np.int16)\n",
    "            \n",
    "            audio_path = os.path.join(output_dir, f'synthetic_audio_{i}.wav')\n",
    "            wavfile.write(audio_path, sample_rate, audio_int16)\n",
    "            \n",
    "            # Generate and save annotations\n",
    "            pauses = audio_processor.detect_pauses(audio)\n",
    "            annotation = {\n",
    "                'audio_file': f'synthetic_audio_{i}.wav',\n",
    "                'sample_rate': sample_rate,\n",
    "                'duration': len(audio) / sample_rate,\n",
    "                'pauses': pauses\n",
    "            }\n",
    "            \n",
    "            json_path = os.path.join(output_dir, 'annotations', f'synthetic_audio_{i}.json')\n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(annotation, f, indent=2)\n",
    "\n",
    "def main(audio_dir, annotation_dir, output_dir, device='cuda'):\n",
    "    dataset = AudioDataset(audio_dir, annotation_dir)\n",
    "    processed_data = dataset.prepare_dataset()\n",
    "    generator, scaler = train_gan(processed_data, device=device)\n",
    "    save_generated_audio_with_annotations(generator, scaler, output_dir, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    main('./cleaned_wav_files', './JSON', './generated_audio', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2785663-5327-43a6-af62-3f78e7cc5a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
